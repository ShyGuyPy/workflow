---
title: "Phyto_progress"
author: "Luke Vawter"
date: "September 18, 2018"
output: html_document
---

Done: 
-get mmir working on my machine
-download CDER and ITIS data
-get pibi_notebook knitting error free on my machine
-read through the majority of pibi_notebook
-learn ggplot basics
-test use of custom function clean_up()
-take a look at scripts in project folder on Zach's old computer(up to date)
-download and work/play with virginia phytoplankton count and event data in Phyto
-download data through pibi_notebook script from url.root <- "http://datahub.chesapeakebay.net/api.JSON"
-do some exploratory data ananysis on phyto data to familiarize yourself both with the data and the EDA process
-edit typos
-send Claire pibi_notebook html by adding Pibi2 to H drive
-figure out where files are being output with fwrite: 
  1. "data/phytoplankton/cedr_phyto_taxa.csv"
  2. "data/phytoplankton/cedr_phyto_event.csv"
  3. "data/phytoplankton/cedr_phyto_station.csv"
  4. "data/water_quality/cedr_wq.csv"
  5. "data/itis/itis_hierarchy.csv"
-replace data_acquistion_cedr with data_input that pulls VA ODU data into phyto2
-get all data in phyto2 going to the right input(done...I think)
-ask Zach about 1) issue with dir.create when knitting pibi_notebook (also knit as start button?)
-figure out why directory/excel is not being generated in workflow when using Zach's code snippet(code snippet works fine outside of r markdown)
-produce an output (xls)
-figure out that loss of salzone data is the problem and search through the pipeline to find out where it's happening(in event prep, salzone letter conversion code, more details in phyto_notes)
-fix whatever is breaking the knit output with the modified sections(was code that was turned into headers rather than fully commented out)
-finish working through R For Data Science(through the parts relevant to phyto at least)
-learn to use extract_notebook_scripts.R to steamline testing process
-figure out where salzones are being lost(prep_events line 123)
-taxa_abund function is in mmir/R/composition.R
-add line in extract script at line 22 to create itit object
-resolve issue with code on line 123 of prep_events(commented out code that was meant to deal with salzone differences...)
-get an ibi score output for VA_ODU data :)
-evaluate ibi metric code to the point where you could effectively bug check it(excluding appendix, I think we can call this one done)
-output ibi.df

-track phyto.df to ibi.df(the primary dataframes throughout phyto):
   -phyto.df
   -bay.df(bay.sub)
   -bay.taxa
   -metrics.df
   -metrics.long(metrics.sub)
   -score_spring_p, score_spring_m, etc.
   -scores.df
   -ratings.df
   -ibi.df
   
-write/rewrite VA_ODU r markdown comments and save to H drive(H/Projects/Phytoplankton IBI)
-after outputting ibi Calire determined that the field crews where listing N for salzone at TF, RET and two LE stations. salzones can't be trusted and need to be recalculated

-figure out why lat and long values in ibi.df output are varying with each output(was jitter being applied by code for leaflet display so points don't fall directly on top of each other)

-know the parts of MMIR and BIBI packages that are applicable to phyto and chessi_bibi(sure)
-recalculate salzones(this part works but now need 3 day window sooner and on event not taxa)

Doing: 
-get 3 day window working on event
-maybe learn parallel package as needed
-re-work/re-build preprocessing section(pdepth not pycnocline)

-re-write/fix any code that was non-functional with VA ODU data where possible(ongoing)

-look through Hadley Wickham's Advanced R guide
-look through r markdown the definitive guide
-take a year of cedr data 1990 and run it through the pipeline





understand steps:

download data and dependencies

-downlaod CEDR data(from API, url.root <- "http://datahub.chesapeakebay.net/api.JSON"
-install packages(including MMIR)
-create vesctor for station data
-download all of the phytoplankton data from the CEDR API


better understanding steps
note:all inputs are case and whitesapce formatted using clean_up before use

DOWNLOAD STATION VECTOR
-create station vector

-Read VA ODU event data to phyto.df

-Filter out data below pycnocline 
or data without taxanomic count	

-using case_when add update to
problematic taxa

-output phyto.df to va odu taxa csv

DOWNLOAD MONITORING EVENT

-download va odu event data to event.df 
and output to csv 

DOWNLOAD STATION INFORMATION

-download station data from cedr api
to station.df and output to csv

DOWNLOAD WATER QUALITY

-download water quality data from cedr 
api to wq.df and output to csv
data is limited to the 3 day window based 
off of dates in phyto.df
the code for the 3 day window grab looks like this:
format(min(phyto.df$sampledate) - days(3), "%m-%d-%Y"),
format(max(phyto.df$sampledate) + days(3), "%m-%d-%Y"), 

PREPARE TXANOMIC HIERARCHY
TAXANOMIC DATA

-taxanomic data uses col.class.vec assigns character type 
to samplenumber, tsn, and speccode

-then assigns taxa csv to taxa.raw

-filter out na and below pycno values
same as was done for events.df

-check for spring and summer months and assigns 
"spring" or "summer" to month

-deal with multiple taxa (I still
don't fully understand this code)

-filter for "ph" in method using grepl
(might be usable)

-also clears taxa deadwood

TAXANOMIC HEIRARCHY

-itis data is read to csv
called hier.wide

-joins hier.wide with bay.df(more 
detail would be nice here)
-is Zach building a unique id out
of the concatination of station,layer, 
samplenumber, sampledate?

-most taxa fixes follow this structure:
if latainname == the string, assign 
second string to the variable at the 
beginning of the statement, otherwise

-removes col.class.vec from environmetn
assign final variable to first

PREPARE CARBON ASSIGNMENTS

-carbon.df is created by reading 
data/carbon/carbon_list_2014

-some cool standardization of data
using str_detect and str_replace_all
(this might be usable)

-carbon.dups is created to hold
taxon that have more than one carbon
value..but carbon.dups is never refereneced 
again(so this is a dead end)

-filter out chaetoceros_wighami(NA),
biddulphia, gymodinium

-not specified and not applicatble are 
converted to NA
-"species" standardized to sp# in bay.df

-creates no.match.df to hold taxa with
 match in carbon.df (using anti_join)
 
 -partial.match...
