---
title: "R_practice3.Rmd"
output: html_document
---
note: continued from R_practice2.Rmd


```{r}
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyr)
library(lubridate)

project.dir <- rprojroot::find_rstudio_root_file()

#import the data
ten_day.df <- data.table::fread(file.path(project.dir, "data/ten_day_test.csv"),
                            data.table = FALSE)

#turn dates to date_time type
ten_day.df$date_time <- as_datetime(as.character(ten_day.df$date_time))
 
#plot the data
ggplot(ten_day.df, aes(x = date_time, y = flow)) + geom_line(aes(linetype = site, colour = site))

#find data with missing flow values
missing.df <- ten_day.df %>%
  dplyr::filter(is.na(flow))
                                
```

```{r}
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyr)
library(lubridate)

project.dir <- rprojroot::find_rstudio_root_file()

#import the data from sarah's site
demands_raw.df <- data.table::fread("http://icprbcoop.org/drupal4/products/coop_pot_withdrawals.csv",
                            data.table = FALSE)

#gather the data into long format
demands.df <- gather(demands_raw.df,site, flow, 2:6)

#turn dates to date_time type
demands.df$DateTime <- as_datetime(as.character(demands.df$DateTime))
 
#plot the data
ggplot(demands.df, aes(x = DateTime, y = flow)) + geom_line(aes(linetype = site, colour = site))

#find data with missing flow values
missing.df <- demands.df %>%
  dplyr::filter(is.na(flow))

```

```{r}
library(RCurl)
test <- url.exists("https://icprbcoop.org/drupal4/products/coop_pot_withdrawals.csv")
print(test)
```

```{r}
library(RCurl)

#site is 
#https://mde.maryland.gov/programs/Water/droughtinformation/Pages/index.aspx

#https://mde.maryland.gov/programs/Water/droughtinformation/Currentconditions/PublishingImages/DroughtGraphsStarting2019jan31/Drought2019-08-31.png

last = 'https://mde.maryland.gov/programs/Water/droughtinformation/Currentconditions/PublishingImages/DroughtGraphsStarting2019jan31/Drought2019-08-31.png' 



# substrRight <- function(x, n){
#   substr(x, nchar(x)-n+1, nchar(x))
# }

# substrRight(x, 6)
# [1] "string"
# 
# substrRight(x, 8)
# [1] "a string"

#grabs the tail end of the address for the Maryland Drought maps last displayed(the date)
last_select = substr(last, nchar(last) -14+1,(nchar(last)-4))

print(last_select)

# makes the date a date object
last_select = as.Date(last_select)

#gets todays date
todays_date = Sys.Date()

#create a vector with all days between today and last date map was updated
dates_array = seq(as.Date(last_select, format="%y-%m-%d"), as.Date(todays_date, format="%y-%m-%d"), by="days")
#dates_array = c(last_select:todays_date)

#print(dates_array[4])

theDate <- last_select +1

test = FALSE


while (theDate <= todays_date && test == FALSE)
{
  url_to_date = (paste0('https://mde.maryland.gov/programs/Water/droughtinformation/Currentconditions/PublishingImages/DroughtGraphsStarting2019jan31/Drought',format(theDate,"20%y-%m-%d"),'.png'))
  
  #https://mde.maryland.gov/programs/Water/droughtinformation/Currentconditions/PublishingImages/DroughtGraphsStarting2019jan31/Drought2019-08-31.png
  
  
  test <- url.exists(url_to_date)
  
  # test <- try(
  #   
  #   url.exists(url_to_date)
  # 
  #   #test =
  #   #download.file(url_to_date, 'url_to_date.png', mode = 'wb')
  #   #return(test)
  # 
  #   # jj <- readPNG('url_to_date.png',native=TRUE)
  #   # plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
  #   # rasterImage(jj,0,0,1,1)
  # 
  # )
  #return(test)

  theDate <- theDate + 1                    
}

print(test)
print(url_to_date)

# jj <- readPNG('url_to_date.png',native=TRUE)
# plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
# rasterImage(jj,0,0,1,1)

```

```{r}
library(RCurl)
#script to get most recent maryland drought map

#url for maryland map always starts this way
map_url_head = 'https://mde.maryland.gov/programs/Water/droughtinformation/Currentconditions/PublishingImages/DroughtGraphsStarting2019jan31/Drought'

#get todays date
todays_date = Sys.Date()

#set test date variable at todays date
test_date = todays_date

#set a test for if the url we search exists
test = FALSE

#while url searched doesn't produce a valid link to map, until then iterate backwards from todays date
while ( test == FALSE)
{
  #concat to produce url in expected format
  url_to_date = (paste0('https://mde.maryland.gov/programs/Water/droughtinformation/Currentconditions/PublishingImages/DroughtGraphsStarting2019jan31/Drought',format(test_date,"20%y-%m-%d"),'.png'))
  
  #tests if
  test <- url.exists(url_to_date)
  
  #back one day
  test_date = test_date -1
}

print(test)
print(url_to_date)
  

```

```{r}
library(RCurl)
library(stringr)

#script to get most recent virginia drought map


map_url_head ='https://deq1.bse.vt.edu/drought/state/images/maps/'

test <- curl::curl(map_url_head)

open(test)

# Get lines
out <- readLines(test)

#print(out)
#for(i in out[12:15]){print(i)}
#for(i in out[9:length(out)]){print(i)}

#for (i in out[length(out):12]){print(i)}

#get todays date
todays_date = Sys.Date()

str_todays_date = as.character(todays_date)
  #substr(todays_date, nchar(todays_date)-10+1, nchar(todays_date))
print(todays_date)
print(typeof(todays_date))
print(str_todays_date)

#reverse iterate through
#for (i in out[length(out):12]){print(i)}

test_loc <- str_locate(out, str_todays_date)#"2019-09-25") 
#print(test_loc)
# print(out[420:425])
# print(typeof(test))

first <- "imageMapFile"

last <- ".png"

id_num<- str_match(entry2, "imageMapFile(.*?).png")
#print(id_num)
full_url = paste0(map_url_head,id_num[1])#,id_num)
print(full_url)
#print(!is.na(test))
# test1 <- (out[50])
# test2 <- (out[200])
# entry1 <- (out[425])
# entry2 <- (out[426])
# 
# str_sub(entry2, 112,140)
# 
# str_sub(test1, 112,140)
# str_sub(test2, 112,140)


# test2 <- curl::curl(paste0(map_url_head,"imageMapFile1569587611359.png"))
# open(test2)


url = paste0(map_url_head,"imageMapFile1569587611359.png")


#"https://deq1.bse.vt.edu/drought/state/images/maps/imageMapFile1569587611359.png"

```

```{r}
#va drought map continued

library(RCurl)
library(stringr)

map_url_head ='https://deq1.bse.vt.edu/drought/state/images/maps/'

test <- curl::curl(map_url_head)

open(test)
out <- readLines(test)

todays_date = Sys.Date()

test_date = todays_date
str_todays_date = as.character(todays_date)

latest_maps <- vector()

#locate values that match date 
test_loc <- str_locate(out, format(test_date,"20%y-%m-%d"))

#######find 
NonNAindex <- which(!is.na(test_loc))
print(NonNAindex)
# print(out[427:439])
# print(out[5526:5568])
# print(out[19952:19970])
# print(out[19952:19970])
# print(out[20410:20422])
# print(out[25509:25551])
print(out[439])

test = out[427]#[427][168:177])
# print(test)

test_full = out[427:439]
# print(test_full)
#use regex to find the id between known elements
id <- str_match(test,'href=\"(.*?)">imageMapFile')
# print(id[,2])
#assemble the full url
full_url = paste0(map_url_head,id[,2])
# print(full_url)
```

```{r}
library(dplyr)
library(tidyverse)
print(out[2])#5592])

#"<tr><td valign=\"top\"><img src=\"/icons/image2.gif\" alt=\"[IMG]\"></td><td><a href=\"imageMapFile15698691306458.png\">imageMapFile15698691306458.png</a></td><td align=\"right\">2019-09-30 14:45  </td><td align=\"right\"> 31K</td><td>&nbsp;</td></tr>"

#print(NonNAindex)

test.df <- data.frame(matrix(ncol = 4, nrow = length(out)))
cols <- c("html","date", "time", "id")
colnames(test.df) <- cols

test3.df <- as.data.frame(out, row.names = NULL, optional = TRUE, col.names = cols )
colnames(test3.df) <- c("html")

id_pattern = 'href=\"(.*?)">imageMapFile'
date_time_pattern = '"right\">(.*?)  </td><td'
full_pattern = *_([^_]*)

test4.df <- test3.df %>% 
  #mutate(id =str_match(html,'href=\"(.*?)">imageMapFile'))
  extract(test.df, html, c("id", "date_time"), regex = id_pattern)

#cbind(test3.df,date, time, id)


#test2.df <- rbind(test.df, out)
print(test2.df)
#lapply(str_match(out,'>2019-09-30 (.*?)  </td><td'))


# for(i in NonNAindex){
#   print(out[i])
# }


  

# for(i in NonNAindex){
#     id <- str_match(out[i],'>2019-09-30 (.*?)  </td><td')
#     test_vec[i] <- id[,2]
# }

print(test_vec)

```

```{r}
test_url <- "<tr><td valign=\"top\"><img src=\"/icons/image2.gif\" alt=\"[IMG]\"></td><td><a href=\"imageMapFile15698691306458.png\">imageMapFile15698691306458.png</a></td><td align=\"right\">2019-09-30 14:45  </td><td align=\"right\"> 31K</td><td>&nbsp;</td></tr>"


#"href=\".*</a></td><td align=\"right\">*</td><td align=\"right\"> 31K</td><td>&nbsp;</td></tr>"

#"(\\d+).*_([^_]*).xlsx"
pattern_id <- "\\w{10,}"
pattern_date <- "\\d{4}[-]\\d{2}[-]\\d{2}"
pattern_time <- "\\d{2}[:]\\d{2}"

test_out <- str_match(test, pattern_time)
```

```{r}
library(RCurl)
library(stringr)
library(dplyr)
library(tidyverse)

#this is the url that contains all the map files
map_url_head ='https://deq1.bse.vt.edu/drought/state/images/maps/'

#creates an object to open 
test <- curl::curl(map_url_head)

#open and read said object
open(test)
out <- readLines(test)

#cols <- c("html")

map.df <- as.data.frame(out, row.names = NULL, optional = TRUE)#, col.names = cols )
colnames(map.df) <- c("html")

######patterns for finding string fragments for retrieving map
pattern_id <- "(\\d{10,})"
pattern_date <- "(\\d{4}[-]\\d{2}[-]\\d{2})"
pattern_time <- "(\\d{2}[:]\\d{2})"

# pattern_b4_time <- "\\D*[^\\w{10,}]"
# pattern_test <- "(\\d{10,})(\\d{4}[-]\\d{2}[-]\\d{2})(\\d{2}[:]\\d{2})"

################


###this is what you want to do but it's not working as is(look at it tomorow)
#test.df <- extract(map.df,html, c("id","date","time"), regex = pattern_test, remove =FALSE)
id.df <- extract(map.df,html, c("id"), regex = pattern_id)
date.df <- extract(map.df,html, c("date"), regex = pattern_date)
time.df <- extract(map.df,html, c("time"), regex = pattern_time)

maps_full.df <- cbind(map.df,id.df, date.df,time.df)

maps_full.df <- arrange(maps_full.df, desc(date),desc(time))

map_url_id = maps_full.df$id[1]
map_url_date = maps_full.df$date[1]
map_url_time = maps_full.df$time[1]
# print(map_url_id)
# print(map_url_date)
# print(map_url_time)
map_url_full = as.character(paste0(map_url_head,"imageMapFile", map_url_id,".png"))
print(map_url_full)
# maps_full <- left_join(id.df, date.df, by = c("html"))
# maps_full <- left_join(maps_full, time.df, by = c("html"))

# maps_full.df$datetime<- as.character(paste(date.df$date,time.df$time))
# 
# test_maps_full.df <- maps_full.df %>%
#   mutate(datetime = case_when(!is.na(datetime)~ as.POSIXct(datetime), TRUE ~ datetime ))

# Test_maps_full.df <- maps_full.df %>%
#   mutate(datetime = case_when(is.na(datetime)~0) )#, TRUE ~ datetime)


# print (min(maps_full.df$datetime))
# 
# 
# 
# 
# 
# 
# print(head(map2.df$date, 430))
```
```{r}
library(RCurl)

site_url <- "https://icprbcoop.org/drupal4/products/coop_pot_withdrawals.csv"

test <- curl::curl(site_url)

open(test)
out <- readLines(test)

print(head(out, 20))

```

```{r}
library(rsconnect)
rsconnect::deployApp("C:/Users/icprbadmin/Documents/R_Scripts/shiny_test_app4")
```

load_packages.R from shiny_test_app4
(use to restore if needed)
```{r}
# List of packages and code to load packages was copied from Zach's code.
# He copied the scripts from:
# https://www.r-bloggers.com/install-and-load-missing-specifiedneeded-packages-on-the-fly/
#
need <- c("shiny",
          "shinythemes",
          "shinydashboard",
          "ggplot2",
          "dplyr",
          "rlang",
          "data.table",
          "stringi",
          #"plotly",
          "Cairo",
          "RcppRoll",
          "tidyr",
          "lubridate",
          "pryr",
          "zoo",
          "sp",
          "leaflet",
          "rgdal",
          "RCurl",
          "tidyverse") 
# find out which packages are installed
ins <- installed.packages()[, 1] 
# check if the needed packages are installed
(Get <-
    need[which(is.na(match(need, ins)))]) 
# install the needed packages if they are not-installed
if (length(Get) > 0) {
  install.packages(Get)
}
# load the needed packages
eval(parse(text = paste("library(", need, ")")))
rm(Get, ins, need)
#------------------------------------------------------------------------------
options(shiny.usecairo = TRUE)
```

```{r}
library(curl)
library(jsonlite)

url_head <- 'https://deq1.bse.vt.edu/drought/state/images/maps/'

# url_test <-'https://deq1.bse.vt.edu/drought/state/images/maps/PrecipMap14050126519712.png'

deploy_test <- curl::curl(url_head)

#print(deploy_test)

out <- readLines(deploy_test)
print(out[1:30])

out_mem <- curl_fetch_memory(url_head)

#parse_headers(out_mem$headers)#[15:30])

#prettify(
rawToChar(out_mem$content)
#)

# tmp <- tempfile()
# curl_download(url_test, tmp)
# readline(tmp)
```

```{r}

test_png = file.path('C:\\Users\\icprbadmin\\Documents\\R_Scripts\\workflow\\data\\DREX\\images\\test_bulb.png')
#'C:/Users/icprbadmin/Documents/R_Scripts/workflow/data/DREX/images/test_bulb.png'
#va_drought_placeholder

print(test_png)

library(png)
img <- readPNG(test_png)#test_png, TRUE, FALSE)#system.file("img", test_png, package="png"))

#print(typeof(img))

grid::grid.raster(img)

```

```{r}
#/inputs/parameters/ts/current
https://icprbcoop.org/drupal4/icprb/flow-data?startdate=01%2F01%2F2019&enddate=10%2F24%2F2019&format=daily&submit=Submit

https://icprbcoop.org/drupal4/products/coop_pot_withdrawals.csv

```


```{r}
library(jsonlite)
library(rprojroot)
library(dplyr)
library(tidyr)

url.root <- "http://datahub.chesapeakebay.net/api.JSON"
todays.date <- format(Sys.Date(), "%m-%d-%Y")

station.vec <- file.path(url.root,
                       "LivingResources",
                       "TidalPlankton",
                       "Reported",
                       min_date,
                       max_date, 
                       phyto_num,
                       "Station") %>% 
  fromJSON() %>% 
  pull(unique(MonitoringLocationId))

clean_string <- function(x) {
  x %>% 
    stringr::str_trim() %>% 
    tolower() %>% 
    stringr::str_replace_all("\\s+", " ") %>% 
    stringr::str_replace_all(" ", "_") %>%  
    if_else(. == "", as.character(NA), .)
}

clean_up <- function(x) {
  x %>% 
    rename_all(clean_string) %>% 
    mutate_if(is.character, funs(clean_string))%>% 
    distinct()
}

min_date = "1-01-1970"

max_date = todays.date

pico_num = "18"

phyto_num = "17"

chla=21
doc=34 
pheo=74 
salinity=83

event.df <- file.path(url.root,
                      "LivingResources",
                      "TidalPlankton",
                      "MonitorEvent",
                      min_date,
                      max_date, 
                      phyto_num,
                      "Station",
                      paste(station.vec, collapse = ",")) %>%
  fromJSON() %>% 
  clean_up()
```

```{r}
library(data.table)
library(dplyr)

demands_raw.df <- data.table::fread("https://icprbcoop.org/drupal4/products/coop_pot_withdrawals.csv",data.table = FALSE)
```


```{r}
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyr)
library(lubridate)

#import the data from sarah's site
demands_raw.df <- data.table::fread("http://icprbcoop.org/drupal4/products/coop_pot_withdrawals.csv",
                                    data.table = FALSE)

#gather the data into long format
demands.df <- gather(demands_raw.df,site, flow, 2:6)

#turn dates to date_time type
demands.df$DateTime <- as_datetime(as.character(demands.df$DateTime))

#write dataframe to file
write.csv(demands.df, paste(
  "download_data_temp.csv"))

# data.table::fwrite(demands.df,paste(
#   "download_data_temp.csv"))


  
#read file
demands2.df <- data.table::fread(paste(
  "download_data_temp.csv"),
                                data.table = FALSE)

demands2.df$DateTime <- as_datetime(as.character(demands2.df$DateTime))

#plot the data
#button interaction needs to be conditional on data being readable in directory
ggplot(demands2.df, aes(x = DateTime, y = flow)) + geom_line(aes(linetype = site, colour = site))
  
```

```{r}


 #construct file path to access Drupal site flows data
  

  
todays_date = Sys.Date()
print(todays_date)

date_year = format(todays_date, "20%y")
date_month = format(todays_date,"%m")
date_day = format(todays_date,"%d")

print(date_year)
print(date_month)
print(date_day)

#https://icprbcoop.org/drupal4/icprb/flow-data?startdate=11%2F2%2F2019&enddate=11%2F07%2F2019&format=hourly&submit=Submit

###hourly---------------------
date_minus_five = todays_date - 5
hourly_start = paste0(format(date_minus_five,"%m"),"%2F",format(date_minus_five,"%d"), "%2F",format(date_minus_five,"20%y"))
print(hourly_start)

hourly_end = paste0(format(todays_date,"%m"),"%2F",format(todays_date,"%d"), "%2F",format(todays_date,"20%y"))

full_hourly_url = paste0("https://icprbcoop.org/drupal4/icprb/flow-data?","startdate=", hourly_start,"&enddate=" ,hourly_end, "&format=hourly&submit=Submit")

print(full_hourly_url)
#-----------------------------


###daily----------------------
first_of_year = as.Date("2019-1-1")

daily_start = paste0(format(first_of_year,"%m"),"%2F",format(first_of_year,"%d"), "%2F",format(first_of_year,"20%y"))
print(daily_start)

daily_end = paste0(format(todays_date,"%m"),"%2F",format(todays_date,"%d"), "%2F",format(todays_date,"20%y"))

full_daily_url = paste0("https://icprbcoop.org/drupal4/icprb/flow-data?","startdate=", daily_start,"&enddate=" ,daily_end, "&format=daily&submit=Submit")

print(full_daily_url)

#-----------------------------
```

```{r}
library(dplyr)
library(tidyverse)
library(lubridate)

#import the data from sarah's site
demands_raw.df <- data.table::fread('https://icprbcoop.org/drupal4/icprb/flow-data?startdate=11%2F2%2F2019&enddate=11%2F07%2F2019&format=hourly&submit=Submit',
```


```{r}
data.table = FALSE, header = TRUE)

#gather the data into long format
demands.df <- gather(demands_raw.df,site, flow, 2:6)

#turn dates to date_time type
demands.df$DateTime <- as_datetime(as.character(demands.df$DateTime))

#plot the data
#button interaction needs to be conditional on data being readable in directory
ggplot(demands.df, aes(x = DateTime, y = flow)) + geom_line(aes(linetype = site, colour = site))
    
  
```


```{r}
  #write old dataframe to old dataframe location 
  write.csv(withdrawals_actual.df, paste0(ts_path, "download_data_w_old.csv"))
```

```{r}
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyverse)

  #write old dataframe to old dataframe location 
  write.csv(withdrawals_actual.df, paste0(ts_path, "download_data_w_old.csv"))

# Path for current operations
ts_path <- "input/ts/current/" 

# #read file
# withdrawals.df <- data.table::fread(paste0(ts_path, "download_data_w_temp.csv"),
#                                       header = TRUE,
#                                   data.table = FALSE)

withdrawals.df <- data.table::fread("http://icprbcoop.org/drupal4/products/coop_pot_withdrawals.csv",
                                      header = TRUE,
                                      na.strings = c("eqp", "Ice", "Bkw", "", "#N/A", -999999),
                                      data.table = FALSE) #, colClasses = list_gage_locations)

#holds the values in wide format and addes dummy rows for formatting purposes
withdrawals_actual.df <-  withdrawals.df %>%
  add_row(DateTime = rep("dummy-row", 10), .before=1)

#gather the data into long format for plotting
withdrawals.df <- gather(withdrawals.df,site, flow, 2:6)

#select only essential(prevents duplicate numbering v1 column)
withdrawals.df <- withdrawals.df %>%
    select(c(DateTime, site, flow))



#if older data exist in directory
if(file.exists(paste0(ts_path, "download_data_w_old.csv"))){
    #grab old withdrawal data from app
withdrawals_old.df <- data.table::fread(paste0(ts_path, "download_data_w_old.csv"),
                                            header = TRUE,
                                        data.table = FALSE)

withdrawals_old.df <- withdrawals_old.df%>%
slice(11:n())

withdrawals_old.df <- withdrawals_old.df %>%
    select(-V1)

#gather the data into long format for plotting
withdrawals_old.df <- gather(withdrawals_old.df,site, flow, 2:6)
    
withdrawals_old.df <- withdrawals_old.df %>%
    select(c(DateTime, site, flow))

    #change site names to be unique from new data (e.g 'lfalls' becomes 'lfalls_old')
withdrawals_old.df <- withdrawals_old.df %>%
      mutate( site =  paste0(site, "_old"))
    
    #turn dates to date_time type
withdrawals_old.df$DateTime <- as_datetime(as.character(withdrawals_old.df$DateTime))
  } else #if old data not in directory 
    {
    #make withdrawals old an empty dataframe
  withdrawals_old.df <-  data.frame(Date=as.Date(character()),
                                           File=character(), 
                                           User=character(), 
                                           stringsAsFactors=FALSE) 
  }
  
  #turn dates to date_time type
withdrawals.df$DateTime <- as_datetime(as.character(withdrawals.df$DateTime))
  
# withdrawals_old.df <- withdrawals_old.df %>%
#     select(c(DateTime, site, flow))
#   
# withdrawals.df <- withdrawals.df %>%
#     select(c(DateTime, site, flow))
  
  #plot the data
  #button interaction needs to be conditional on data being readable in directory
if(file.exists(paste0(ts_path, "download_data_w_old.csv"))){
  
  withdrawals_new_and_old.df <- full_join(withdrawals.df, withdrawals_old.df)#, by = c(DateTime, site, flow))
  
  ggplot(withdrawals_new_and_old.df, aes(x = DateTime, y = flow)) + geom_line(aes(linetype = site, colour = site)) 
} else {
  ggplot(withdrawals.df, aes(x = DateTime, y = flow)) + geom_line(aes(linetype = site, colour = site))
}
    
```

```{r}
#####################withdrawals download data
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyverse)

# Path for current operations
ts_path <- "input/ts/current/" 


withdrawals.df <- data.table::fread("http://icprbcoop.org/drupal4/products/coop_pot_withdrawals.csv",
                                      header = TRUE,
                                      na.strings = c("eqp", "Ice", "Bkw", "", "#N/A", -999999),
                                      data.table = FALSE) 

withdrawals_actual.df <-  withdrawals.df

  #writes wide/formatted data
write.csv(withdrawals_actual.df, paste0(ts_path, "download_data_w_actual.csv"))
  
# withdrawals.df <- withdrawals.df %>%
#     select(-V1)
  
  #gather the data into long format for plotting
withdrawals.df <- gather(withdrawals.df,site, flow, 2:6)

  #select only essential(prevents duplicate numbering v1 column)
withdrawals.df <- withdrawals.df %>%
    select(c(DateTime, site, flow))
  
  # #turn dates to date_time type
  # withdrawals.df$DateTime <- as_datetime(as.character(withdrawals.df$DateTime))
  
  #write dataframe to file
write.csv(withdrawals.df, paste0(ts_path, "download_data_w_temp.csv"))
```  



```{r}
################withdrawals view data
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyverse)

# Path for current operations
ts_path <- "input/ts/current/" 


  #read file
  withdrawals.df <- data.table::fread(paste0(ts_path, "download_data_w_temp.csv"),
                                      header = TRUE,
                                      na.strings = c("eqp", "Ice", "Bkw", "", "#N/A", -999999),
                                      data.table = FALSE) 
  
  #select only essential(prevents duplicate numbering v1 column)
  withdrawals.df <- withdrawals.df %>%
    select(c(DateTime, site, flow))
  
#if older data exist in directory
if(file.exists(paste0(ts_path, "download_data_w_old.csv"))){
  #grab old withdrawal data from app
  withdrawals_old.df <- data.table::fread(paste0(ts_path, "download_data_w_old.csv"),
                                          header = TRUE,
                                      data.table = FALSE)
  
  # withdrawals_old.df <- withdrawals_old.df %>%
  #   select(-V1)
  
  #gather the old data into long format
  withdrawals_old.df <- gather(withdrawals_old.df,site, flow, 2:6)
  
  #select only essential(prevents duplicate numbering v1 column)
  withdrawals_old.df <- withdrawals_old.df %>%
    select(c(DateTime, site, flow))
  
  #change site names to be unique from new data (e.g 'lfalls' becomes 'lfalls_old')
  withdrawals_old.df <- withdrawals_old.df %>%
    mutate( site =  paste0(site, "_old"))
  
  #turn dates to date_time type
  withdrawals_old.df$DateTime <- as_datetime(as.character(withdrawals_old.df$DateTime))
  

  
} 

#change DateTime column format to as_datetime
withdrawals.df$DateTime <- as_datetime(as.character(withdrawals.df$DateTime))

if(file.exists(paste0(ts_path, "download_data_w_old.csv"))){
  
  withdrawals_new_and_old.df <- full_join(withdrawals.df, withdrawals_old.df)
  
  test_plot <- ggplot(withdrawals_new_and_old.df, aes(x = DateTime, y = flow)) + geom_line(aes(linetype = site, colour = site)) 
  
} else {
  test_plot <- ggplot(withdrawals.df, aes(x = DateTime, y = flow)) + geom_line(aes(linetype = site, colour = site))
  
}  

```
```{r}
##################withdrawals accept data
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyverse)

# Path for current operations
ts_path <- "input/ts/current/" 

  #read old data file
  withdrawals.df <- data.table::fread(paste0(ts_path, "coop_pot_withdrawals.csv"),
                                  data.table = FALSE)
  
  withdrawals.df <- withdrawals_actual.df

  
  #write old dataframe to old dataframe location 
  write.csv(withdrawals.df, paste0(ts_path, "download_data_w_old.csv"), row.names=FALSE)
  
  #read temp file(grabbing the data that has added dummy rows and is still in wide format)
  withdrawals.df <- data.table::fread(paste0(ts_path, "download_data_w_actual.csv"),
                                  data.table = FALSE)
  #overwrite dataframe to latest(active) data position
  write.csv(withdrawals.df, paste0(ts_path, "coop_pot_withdrawals.csv"), row.names=FALSE)

```


```{r}
#########################flows daily download data
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyverse)

# Path for current operations
ts_path <- "input/ts/current/" 

#todays date
date_today0 <- as.Date(today())

parameters_path <- "input/parameters/"

gages <- data.table::fread(paste(parameters_path, "gages.csv", sep = ""),
                           col.names = c("id", "location", "description"),
                                         data.table = FALSE)
#list of gages
list_gage_locations <- c("date", gages$location)

#construct path for daily flows data
first_of_year = as.Date("2019-1-1")

daily_start = paste0(format(first_of_year,"%m"),"%2F",format(first_of_year,"%d"), "%2F",format(first_of_year,"20%y"))
print(daily_start)

daily_end = paste0(format(date_today0,"%m"),"%2F",format(date_today0,"%d"), "%2F",format(date_today0,"20%y"))

full_daily_url = paste0("https://icprbcoop.org/drupal4/icprb/flow-data?","startdate=", daily_start,"&enddate=" ,daily_end, "&format=daily&submit=Submit")



#import the data from sarah's site
flows_daily.df <- data.table::fread(full_daily_url, header = TRUE,
                                    data.table = FALSE,   colClasses = c("character", rep("numeric", 31)), # force cols 2-32 numeric
                                    na.strings = c("eqp", "Ice", "Bkw", "", "#N/A", -999999),
                                    col.names = list_gage_locations)

#holds the values in wide format and addes dummy rows for formatting purposes
flows_daily_actual.df <-  flows_daily.df #%>%
#   add_row(DateTime = rep("dummy-row", 10), .before=1)

#writes wide/formatted data
write.csv(flows_daily_actual.df, paste0(ts_path, "download_data_fd_actual.csv"))


# flows_daily.df <- flows_daily.df %>%
#   select(-V1)

#gather the data into long format for plotting
flows_daily.df <- gather(flows_daily.df,site, flow, 2:6)

#select only essential(prevents duplicate numbering v1 column)
flows_daily.df <- flows_daily.df %>%
  select(c(date, site, flow))

# #turn dates to date_time type
# flows_daily.df$DateTime <- as_datetime(as.character(flows_daily.df$DateTime))

#write dataframe to file
write.csv(flows_daily.df, paste0(ts_path, "download_data_fd_temp.csv"))
  


```


```{r}
#########################flows daily view data
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyverse)

# Path for current operations
ts_path <- "input/ts/current/" 

#todays date
date_today0 <- as.Date(today())

parameters_path <- "input/parameters/"

gages <- data.table::fread(paste(parameters_path, "gages.csv", sep = ""),
                           col.names = c("id", "location", "description"),
                                         data.table = FALSE)
#list of gages
list_gage_locations <- c("date", gages$location)

#construct path for daily flows data
first_of_year = as.Date("2019-1-1")




  #read file
flows_daily.df <- data.table::fread(paste0(ts_path, "download_data_fd_temp.csv"),
                                      header = TRUE,
                                  data.table = FALSE)
  
flows_daily.df <- flows_daily.df %>%
select(-V1)

#select only essential(prevents duplicate numbering v1 column)
flows_daily.df <- flows_daily.df %>%
  select(c(date, site, flow))


#if older data exist in directory
if(file.exists(paste0(ts_path, "download_data_fd_old.csv"))){
  #grab old flows daily data from app
  flows_daily_old.df <- data.table::fread(paste0(ts_path, "download_data_fd_old.csv"),
                                          header = TRUE,
                                          data.table = FALSE)
  
#gather the old data into long format
flows_daily_old.df <- gather(flows_daily_old.df,site, flow, 2:32)
  
#select only essential(prevents duplicate numbering v1 column)
flows_daily_old.df <- flows_daily_old.df %>%
  select(c(date, site, flow))

#change site names to be unique from new data (e.g 'lfalls' becomes 'lfalls_old')
flows_daily_old.df <- flows_daily_old.df %>%
  mutate( site =  paste0(site, "_old"))

#turn dates to date_time type
flows_daily_old.df$date <- as_datetime(as.character(flows_daily_old.df$date))


}

# 
#turn dates to date_time type
flows_daily.df$date <- as_datetime(as.character(flows_daily.df$date))

#filter down to only 2 sites
flows_daily.df <-  flows_daily.df %>%
  filter(site == "lfalls" | site == "por")
  
  #plot the data
  # #button interaction needs to be conditional on data being readable in directory
  # output$flows_daily_plot <- renderPlot({ ggplot(flows_daily.df, aes(x = date, y = flow)) + geom_line(aes(linetype = site, colour = site))
    # })
  
  if(file.exists(paste0(ts_path, "download_data_fd_old.csv"))){
    
    flows_daily_new_and_old.df <- full_join(flows_daily.df, flows_daily_old.df)#, by = c(DateTime, site, flow))
    
    output$withdrawal_plot <- ggplot(flows_daily_new_and_old.df, aes(x = date, y = flow)) + geom_line(aes(linetype = site, colour = site)) 

  } else {
ggplot(flows_daily.df, aes(x = date, y = flow)) + geom_line(aes(linetype = site, colour = site))

  }
```

```{r}
############# accept and save flows daily data


#read old data file
flows_daily.df <- data.table::fread(paste0(ts_path, "flows_daily_cfs.csv"),
                                    header = TRUE,
                                    data.table = FALSE)

# flows_daily.df <- flows_daily.df %>%
#     select(-V1)

#write old dataframe to old dataframe location 
write.csv(flows_daily.df, paste0(ts_path, "download_data_fd_old.csv"), row.names=FALSE)
  
#read temp file
flows_daily.df <- data.table::fread(paste0(ts_path, "download_data_fd_actual.csv"),
                                    header = TRUE,
                                data.table = FALSE,)
  
  #overwrite dataframe to latest(active) data position
write.csv(flows_daily.df, paste0(ts_path, "flows_daily_cfs.csv"), row.names=FALSE)
```

```{r}

### flows hourly download data

  #construct file path
  date_minus_five = date_today0 - 5
  hourly_start = paste0(format(date_minus_five,"%m"),"%2F",format(date_minus_five,"%d"), "%2F",format(date_minus_five,"20%y"))
  
  hourly_end = paste0(format(date_today0,"%m"),"%2F",format(date_today0,"%d"), "%2F",format(date_today0,"20%y"))
  
  full_hourly_url = paste0("https://icprbcoop.org/drupal4/icprb/flow-data?","startdate=", hourly_start,"&enddate=" ,hourly_end, "&format=hourly&submit=Submit")
  
  #import the hourly flows data from sarah's site
  flows_hourly.df <- data.table::fread(full_hourly_url, header = TRUE,
                                      data.table = FALSE, colClasses = c("character", rep("numeric", 31)), # force cols 2-32 numeric
                                      na.strings = c("eqp", "Ice", "Bkw", "", "#N/A", -999999),
                                      col.names = list_gage_locations)
  
  #holds the values in wide format and addes dummy rows for formatting purposes
  flows_hourly_actual.df <-  flows_hourly.df #%>%
  #   add_row(DateTime = rep("dummy-row", 10), .before=1)
  
  #writes wide/formatted data
  write.csv(flows_hourly_actual.df, paste0(ts_path, "download_data_fh_actual.csv"), row.names=FALSE)
  
  # flows_hourly.df <- flows_hourly.df %>%
  #   select(-V1)
  
  #gather the data into long format for plotting
  flows_hourly.df <- gather(flows_hourly.df,site, flow, 2:6)
  
  #select only essential(prevents duplicate numbering v1 column)
  flows_hourly.df <- flows_hourly.df %>%
    select(c(date, site, flow))
  
  # #turn dates to date_time type
  # flows_hourly.df$DateTime <- as_datetime(as.character(flows_hourly.df$DateTime))
  
  #write dataframe to file
  write.csv(flows_hourly.df, paste0(ts_path, "download_data_fh_temp.csv"), row.names=FALSE)
  
```

```{r}

### flows hourly view data

  #read file
  flows_hourly.df <- data.table::fread(paste0(ts_path, "download_data_fh_temp.csv"),
                                       header = TRUE,
                                  data.table = FALSE)
  
  #select only essential(prevents duplicate numbering v1 column)
  flows_hourly.df <- flows_hourly.df %>%
    select(c(date, site, flow))
  
  #if older data exist in directory
  if(file.exists(paste0(ts_path, "download_data_fh_old.csv"))){
    #grab old flows hourly data from app
    flows_hourly_old.df <- data.table::fread(paste0(ts_path, "download_data_fh_old.csv"),
                                             header = TRUE,
                                            data.table = FALSE)
    
        #gather the data into long format
    flows_hourly_old.df <- gather(flows_hourly_old.df,site, flow, 2:32)
    
    #select only essential(prevents duplicate numbering v1 column)
    flows_hourly_old.df <- flows_hourly_old.df %>%
      select(c(date, site, flow))
    
    #change site names to be unique from new data (e.g 'lfalls' becomes 'lfalls_old')
    flows_hourly_old.df <- flows_hourly_old.df %>%
      mutate( site =  paste0(site, "_old"))
    
    #turn dates to date_time type
    flows_hourly_old.df$date <- as_datetime(as.character(flows_hourly_old.df$date))
    

  }
  
  #turn dates to date_time type
  flows_hourly.df$date <- as_datetime(as.character(flows_hourly.df$date))
  
  #filter down to only 2 sites
  flows_hourly.df <-  flows_hourly.df %>%
    filter(site  == "lfalls" | site == "por")
  
  #plot the data
  #button interaction needs to be conditional on data being readable in directory
  # output$flows_hourly_plot <- renderPlot({ ggplot(flows_hourly.df, aes(x = date, y = flow)) + geom_line(aes(linetype = site, colour = site))
  #})
  
  if(file.exists(paste0(ts_path, "download_data_fh_old.csv"))){
    
    flows_hourly_new_and_old.df <- full_join(flows_hourly.df, flows_hourly_old.df)#, by = c(DateTime, site, flow))
    
ggplot(flows_hourly_new_and_old.df, aes(x = date, y = flow)) + geom_line(aes(linetype = site, colour = site)) 

  } else {
ggplot(flows_hourly.df, aes(x = date, y = flow)) + geom_line(aes(linetype = site, colour = site))
   
  }
```

```{r}
############# accept and save flows hourly data

  #read old data file
  flows_hourly.df <- data.table::fread(paste0(ts_path, "flows_hourly_cfs.csv"),
                                      data.table = FALSE)
  #write old dataframe to old dataframe location 
  write.csv(flows_hourly.df, paste0(ts_path, "download_data_fh_old.csv"), row.names=FALSE)
  
  #read temp file(grabbing the data that has added dummy rows and is still in wide format)
  flows_hourly.df <- data.table::fread(paste0(ts_path, "download_data_fh_actual.csv"),
                                      data.table = FALSE)
  
  ################# need code to append new data to old data
  ######requires a join to existing data
  
  #write dataframe to file
  write.csv(flows_hourly.df, paste0(ts_path, "flows_hourly_cfs.csv"), row.names=FALSE)
  
```

```{r}

library(dplyr)
library(tidyr)
library(data.table)
library(lubridate)
library(readxl)
library(jsonlite)
library(rprojroot)
library(ritis)
library(stringr)
library(purrr)
library(lubridate)

url.root <- "http://datahub.chesapeakebay.net/api.JSON"
todays.date <- format(Sys.Date(), "%m-%d-%Y")


#this is the minimum date to be included
min_date = "01-01-2018"#

#this is the maximum date to be included, add the variable todays.date into this variable if you want the most recent data
max_date = todays.date#
#------------------------

#------number for phytoplankton in CEDR api-------------------
phyto_num = "17"


#------number for picoplankton in CEDR api--------------------
pico_num = "18"
#-------------------------------------------------------------

#------number for phytoplankton in CEDR api-------------------
phyto_num = "17"
#-------------------------------------------------------------

#------these are the parameter codes for the CEDR api---------
chla=21
doc=34 
pheo=74 
salinity=83
#-------------------------------------------------------------

#-----------------clean data----------------------------
#should be in functions
clean_string <- function(x) {
  x %>% 
    stringr::str_trim() %>% 
    tolower() %>% 
    stringr::str_replace_all("\\s+", " ") %>% 
    stringr::str_replace_all(" ", "_") %>%  
    if_else(. == "", as.character(NA), .)
}

clean_up <- function(x) {
  x %>% 
    rename_all(clean_string) %>% 
    mutate_if(is.character, funs(clean_string))%>% 
    distinct()
}
#----------------------------------------------------



#-----create a station vector for us in data pull------
#temporary placement
station.vec <- file.path(url.root,
                         "LivingResources",
                         "TidalPlankton",
                         "Reported",
                         min_date,
                         max_date,
                         phyto_num,
                         "Station") %>%
  fromJSON() %>%
  pull(unique(MonitoringLocationId))

print(station.vec)

# station.vec   %>%
#   fromJSON() %>%
#   dplyr::pull(unique(MonitoringLocationId))

data.df <- file.path(url.root,
                     "LivingResources",
                     "TidalPlankton",
                     "Reported",
                     min_date,
                     max_date,
                     phyto_num,
                     "Station",
                     paste(station.vec, collapse = ",")) %>%
  fromJSON() %>%
  clean_up()

#print(max_date)
```

```{r}
library(reticulate)


```

```{python}
import pandas as pd

test_data = {'t1': ['test1test','test2test'],
             't2': ['testone','testtwo']
             }


data_df = pd.DataFrame(data=test_data)
#print(data_df)
data_df['t3']= [data_df.iloc[0,0][-5:-4],data_df.iloc[1,0][-5:-4]]
# test = data_df.iloc[0,1]
# print(test)

data_df =pd.concat([data_df,pd.DataFrame(columns=("coltest1", "coltest2") ##(columns=list('ABCD')
                                         )], sort=False)

butter = 333

data_df.iloc[0, data_df.columns.get_loc('t2')] = butter

data_df['coltest1'] = data_df['t1']

print(data_df)

```


```{r}
library(data.table)
library(dplyr)
library(tidyverse)

ts_path <- "input/ts/current/" 

withdrawals_raw.df <- data.table::fread("http://icprbcoop.org/drupal4/products/coop_pot_withdrawals.csv",
                                      header = TRUE,
                                      na.strings = c("eqp", "Ice", "Bkw", "", "#N/A", -999999),
                                      data.table = FALSE) 

#write test csv 
write_csv(withdrawals_raw.df, paste0(ts_path, "download_data_w_raw.csv"))

withdrawals_correct_format.df <- data.table::fread(paste0(ts_path, "coop_pot_withdrawals.csv"),
                                  data.table = FALSE)


# get add_row to work correctly

 #read temp file(grabbing the data that has added dummy rows and is still in wide format)
withdrawals_pulled.df <- data.table::fread(paste0(ts_path, "download_data_w_actual.csv"),
                                  data.table = FALSE)
#add dummyrows to match format
withdrawals_formatted.df <- withdrawals_raw.df %>%
add_row(DateTime = rep("#dummy-row", 10), .before=1)

#write test csv (currently wrong format)
write_csv(withdrawals_formatted.df, paste0(ts_path, "download_data_w_format_test.csv"))
```

```{r}
library(data.table)
library(dplyr)
library(tidyverse)

ts_path <- "input/ts/current/" 

withdrawals_raw.df <- data.table::fread("http://icprbcoop.org/drupal4/products/coop_pot_withdrawals.csv",
                                      header = TRUE,
                                      na.strings = c("eqp", "Ice", "Bkw", "", "#N/A", -999999),
                                      data.table = FALSE) 

withdrawals_formatted.df <- withdrawals_raw.df %>%
  # write the future header
  add_row(DateTime = "DateTime", FW = "FW", WSSC = "WSSC",
          WA_GF = "WA_GF", WA_LF = "WA_LF", LW = "LW", .before = 1) %>%
  # write 10 dummy rows, to mimic file from the Data Portal
  add_row(DateTime = rep("#dummy-row", 10), .before=1)
  

#write test csv (currently wrong format)
write_csv(withdrawals_formatted.df, paste0(ts_path, "download_data_w_format_test.csv"),col_names = FALSE)

# withdrawals_read.df <- data.table::fread(paste0(ts_path, "download_data_w_format_test.csv"),
#                                   data.table = FALSE,
#                                   header = FALSE) 
# 
# #write test csv (currently wrong format)
# write_csv(withdrawals_read.df, paste0(ts_path, "download_data_w_format_test.csv"))
```

DIME CEDR data pull
```{r}

library(dplyr)
library(tidyr)
library(data.table)
library(lubridate)
library(readxl)
library(jsonlite)
library(rprojroot)
library(ritis)
library(stringr)
library(purrr)
library(lubridate)
library(ggplot2)
library(tidyverse)

data_path <- "data/CEDR/"

url.root <- "http://datahub.chesapeakebay.net/api.JSON"
todays.date <- format(Sys.Date(), "%m-%d-%Y")


#this is the minimum date to be included
min_date = "01-01-2018"#

#this is the maximum date to be included, add the variable todays.date into this variable if you want the most recent data
max_date = todays.date#
#------------------------

#------number for phytoplankton in CEDR api-------------------
phyto_num = "17"


#------number for picoplankton in CEDR api--------------------
pico_num = "18"
#-------------------------------------------------------------

#------number for phytoplankton in CEDR api-------------------
phyto_num = "17"
#-------------------------------------------------------------



#------these are the parameter codes for the CEDR api---------
chla=21
doc=34 
pheo=74 
salinity=83
#-------------------------------------------------------------

#-----------------clean data----------------------------
#should be in functions
clean_string <- function(x) {
  x %>% 
    stringr::str_trim() %>% 
    tolower() %>% 
    stringr::str_replace_all("\\s+", " ") %>% 
    stringr::str_replace_all(" ", "_") %>%  
    if_else(. == "", as.character(NA), .)
}

clean_up <- function(x) {
  x %>% 
    rename_all(clean_string) %>% 
    mutate_if(is.character, funs(clean_string))%>% 
    distinct()
}
#----------------------------------------------------



#-----create a station vector for us in data pull------
#temporary placement
station.vec <- file.path(url.root,
                         "LivingResources",
                         "TidalPlankton",
                         "Reported",
                         min_date,
                         max_date,
                         phyto_num,
                         "Station") %>%
  fromJSON() %>%
  pull(unique(MonitoringLocationId))

print(station.vec)

# station.vec   %>%
#   fromJSON() %>%
#   dplyr::pull(unique(MonitoringLocationId))


#----------------pull phyto--------------------------------
# plot_data.df <- file.path(url.root,
#                      "LivingResources",
#                      "TidalPlankton",
#                      "MonitorEvent",
#                      min_date,
#                      max_date,
#                      phyto_num,
#                      "Station",
#                      paste(station.vec, collapse = ",")) %>%
#   fromJSON() %>%
#   clean_up()
#------------------------------------------------------------

#-------------------pull water quality-------------------------
plot_data.df <- file.path(url.root,
                   "WaterQuality",
                   "WaterQuality",

                   min_date,
                   max_date,
                   "6",  #programIds
                   "7,23",#"7,16,23,24",  #projectIds
                   "station",
                   paste(station.vec, collapse = ","),
                   paste(
                     #these are the parameter variables declared in Functions and Variables section
                     chla,doc,pheo,salinity
                     , sep=",")) %>%
  fromJSON() %>%
  clean_up()
#---------------------------------------------------------------

write_csv(plot_data.df, paste0(data_path, "download_data_water_quality.csv"))#,col_names = FALSE)

#----------------plot data-------------------

data_path <- "data/CEDR/"

#read file
plot_data.df <- data.table::fread(paste0(data_path, "download_data_water_quality.csv"),
                                    header = TRUE,
                                    data.table = FALSE)
  

ggplot(plot_data.df, aes(x = sampledate, y = measurevalue)) + geom_line(aes(linetype = layer, colour = layer))  


data_na_count <- as.numeric(sum(is.na(plot_data.df$measurevalue)))
print(data_na_count)

data_num <- nrow(plot_data.df)
data_na_percent <- (data_na_count / data_num)




```

```{r}
MAIN = 7 #Tidal Mainstem Water Quality Monitoring Project
TRIB = 23 #Tidal Tributary Water Quality Monitoring Project


test <- paste(MAIN,TRIB
                           , sep=",")

print(test)
```

```{r}
library(shiny)
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyr)
library(lubridate)

data_path <- "data/CEDR/"

data_tweak.df <- data.table::fread(paste0(data_path, "cedr_wq_tweak.csv"),
                                   data.table = FALSE)

data_tweak_test.df <- data_tweak.df %>%
  mutate(sampledatetime = paste0(substr(sampledate, start = 1, stop = 11), sampletime))



#takes the date from the sampledate object minus the blank time stamp and combines it
# with sampledate to create a proper datetime object
to_date_time <- function(dataframe) {
  dataframe %>%
    mutate(sampledatetime = as.POSIXct(paste0(substr(sampledate, start = 1, stop = 10)," ", sampletime)) )
}

func_test.df <- data_tweak.df %>%
  to_date_time()

print(typeof(func_test.df$sampledatetime))



ggplot(
    func_test.df, aes(x = sampledatetime, y = measurevalue )) + geom_line()
      #aes(linetype = layer, colour = layer)) #+ scale_x_discrete(breaks = seq(0, 100, by = 100)) 
```


working on HUC vectors
```{r}
library(shiny)
library(shinythemes)
library(shinydashboard)
library(tidyverse)
library(dplyr)
library(jsonlite)
library(ggplot2)
library(rlang)
library(data.table)
library(stringi)
library(Cairo)
library(RcppRoll)
library(tidyr)
library(lubridate)
library(pryr)
library(zoo)
library(sp)
library(leaflet)
library(rgdal)
library(RCurl)
library(curl)

#-----CEDR api url-------------------------------------
CEDR_url <- "http://datahub.chesapeakebay.net/api.JSON"
#------------------------------------------------------

#-------------------todays date------------------------
todays.date <- format(Sys.Date(), "%m-%d-%Y")
#------------------------------------------------------

#------number for phytoplankton in CEDR api-------------------
phyto_num = "17"
#-------------------------------------------------------------

#this is the minimum date to be included
min_date = "12-13-2018"#1970"#

#this is the maximum date to be included, add the variable todays.date into this variable if you want the most recent data
max_date = todays.date#

CEDR_url <- "http://datahub.chesapeakebay.net/api.JSON"
todays.date <- format(Sys.Date(), "%m-%d-%Y")

MAIN = 7 #Tidal Mainstem Water Quality Monitoring Project
TRIB = 23 #Tidal Tributary Water Quality Monitoring Project

#----------------water quality parameters-----------------------
CLW =23
DIN = 30
DO = 31
DOC = 34
DON = 35
DOP = 36
NH4F = 60
NO23F = 63
NO2F = 65
PC =  71
PH = 73
PN =  77
PO4F = 78
PP =82
SALINITY = 83
SIGMA_T = 88
SO4W = 93
SPCOND = 94
TALK = 100
TDN = 104
TDP = 105
TN = 109
TON = 111
TP = 114
TSS = 116
TURB_NTU = 119
WTEMP = 123

#----------------------------------------------------------------

clean_string <- function(dataframe) {
  dataframe %>% 
    stringr::str_trim() %>% 
    tolower() %>% 
    stringr::str_replace_all("\\s+", " ") %>% 
    stringr::str_replace_all(" ", "_") %>%  
    if_else(. == "", as.character(NA), .)
}

clean_up <- function(dataframe) {
  dataframe %>% 
    rename_all(clean_string) %>% 
    mutate_if(is.character, funs(clean_string))%>% 
    distinct()
}


#-------------------location vector-------------------
huc8.vec <- file.path(CEDR_url,
                         "LivingResources",
                         "TidalPlankton",
                         "Reported",
                         min_date,
                         max_date,
                         phyto_num,
                         #"Station"
                         "Huc8"
                         ) %>%
  fromJSON() %>%
  pull(unique(HUCEightId))
    #MonitoringLocationId
#---------------------------------------------------


#-------------------data pull-------------------------
        plot_data.df <- file.path(CEDR_url,
                                  "WaterQuality",
                                  "WaterQuality",
                                  
                                  min_date,
                                  max_date,
                                  "6",  #programIds
                                  paste(
                                          #these are the parameter variables declared in Functions and Variables section
                                          MAIN,TRIB
                                          , sep=","),#"7,16,23,24",  #projectIds
                                  "Huc8",
                                  paste(huc8.vec, collapse = ","),
                                  paste(
                                          #these are the parameter variables declared in Functions and Variables section
                                          CLW, DIN, DO, DOC, DON, DOP, NH4F, NO23F, NO2F, PC, PH, PN, PO4F, PP, SALINITY, SIGMA_T, SO4W, SPCOND, TALK, TDN, TDP, TN, TON, TP, TSS, TURB_NTU, WTEMP
                                          , sep=",")
        ) %>%
                fromJSON() %>%
                clean_up() #%>%
#-----------------------------------------------------

#average by location and parameter
###write as function for prep for mapping in CEDR_PULL
  data_map.df <- plot_data.df %>%
    mutate(sampledatetime = as.POSIXct(paste0(substr(sampledate, start = 1, stop = 10)," ", sampletime)) ) %>%
    group_by(huc8, sampledatetime, parameter) %>%
    mutate(measurevalue_averaged = mean(measurevalue)) %>%
    select(huc8, sampledatetime, measurevalue_averaged, parameter ) %>%
    unique()

```



```{r}
library(shiny)
library(shinythemes)
library(shinydashboard)
library(tidyverse)
library(dplyr)
library(jsonlite)
library(ggplot2)
library(rlang)
library(data.table)
library(stringi)
library(Cairo)
library(RcppRoll)
library(tidyr)
library(lubridate)
library(pryr)
library(zoo)
library(sp)
library(leaflet)
library(rgdal)
library(RCurl)
library(curl)

#-------------path to data directory-------------------
data_path <- "data/CEDR/"
#------------------------------------------------------

#-----CEDR api url-------------------------------------
CEDR_url <- "http://datahub.chesapeakebay.net/api.JSON"
#------------------------------------------------------

#-------------------todays date------------------------
todays.date <- format(Sys.Date(), "%m-%d-%Y")
#------------------------------------------------------

#------number for phytoplankton in CEDR api-------------------
phyto_num = "17"
#-------------------------------------------------------------

#this is the minimum date to be included
min_date = "12-13-2018"#1970"#

#this is the maximum date to be included, add the variable todays.date into this variable if you want the most recent data
max_date = todays.date#

CEDR_url <- "http://datahub.chesapeakebay.net/api.JSON"
todays.date <- format(Sys.Date(), "%m-%d-%Y")

MAIN = 7 #Tidal Mainstem Water Quality Monitoring Project
TRIB = 23 #Tidal Tributary Water Quality Monitoring Project

#----------------water quality parameters-----------------------
CLW =23
DIN = 30
DO = 31
DOC = 34
DON = 35
DOP = 36
NH4F = 60
NO23F = 63
NO2F = 65
PC =  71
PH = 73
PN =  77
PO4F = 78
PP =82
SALINITY = 83
SIGMA_T = 88
SO4W = 93
SPCOND = 94
TALK = 100
TDN = 104
TDP = 105
TN = 109
TON = 111
TP = 114
TSS = 116
TURB_NTU = 119
WTEMP = 123

SECCHI = 85
CHLA=21
PHEO=74 
#----------------------------------------------------------------

#--------------------------------data collection problem codes---
A = "LABORATORY ACCIDENT" 
B = "CHEMICAL MATRIX  INTERFERENCE" 
BB  = "TORN FILTER PAD" 
C = "INSTRUMENT FAILURE" 
D = "INSUFFICIENT SAMPLE" 
DD = "SAMPLE SIZE NOT REPORTED (ASSUMED)" 
E = "SAMPLE RECEIVED AFTER HOLDING TIME" 
FF =  "POOR REPLICATION BETWEEN PADS, MEAN REPORTED" 
GG =  "SAMPLE ANALYZED AFTER HOLDING TIME" 
I = "SUSPECT VALUE HAS BEEN VERIFIED CORRECT" 
J =  "INCORRECT SAMPLE FRACTION FOR ANALYSIS" 
JJ = "VOLUME FILTERED NOT RECORDED (ASSUMED)" 
L = "LICOR CALIBRATION OFF BY >= 10% PER YEAR.  USE WITH CALC KD WHERE PROB OF LU,   LS, LB EXIST IN RAW" 
LB = "LICOR CALIBRATION OFF BY >= 10% PER YEAR FOR BOTH AIR AND UPWARD  FACING SENSORS" 
LS =  "LICOR CALIBRATION OFF BY >= 10% PER YEAR FOR AIR SENSOR" 
LU = "LICOR CALIBRATION OFF BY >= 10% PER YEAR FOR UPWARD FACING SENSOR" 
MM = "OVER 20% OF SAMPLE ADHERED TO POUCH AND OUTSIDE OF PAD" 
NN = "PARTICULATES FOUND IN FILTERED SAMPLE"   
P = "PROVISIONAL DATA" 
QQ = "PART EXCEEDS WHOLE VALUE YET DIFFERENCE IS WITHIN ANALYTICAL PRECISION" 
R = "SAMPLE CONTAMINATED" 
RR = "NO SAMPLE RECEIVED"                                                                                   
SS = "SAMPLE REJECTED, HIGH SUSPENDED SEDIMENT CONCENTRATION" 
U = " MATRIX PROBLEM RESULTING FROM THE INTERRELATIONSHIP BETWEEN VARIABLES SUCH AS PH AND AMMONIA"         
V = "SAMPLE RESULTS REJECTED DUE TO QC CRITERIA" 
VV = "STATION WAS NOT SAMPLED DUE TO BAD FIELD CONDITIONS" 
WW = "HIGH OPTICAL DENSITY (750 NM); ACTUAL VALUE RECORDED" 
X = "SAMPLE NOT PRESERVED PROPERLY"
#-----------------------------------------------------------------

clean_string <- function(dataframe) {
  dataframe %>% 
    stringr::str_trim() %>% 
    tolower() %>% 
    stringr::str_replace_all("\\s+", " ") %>% 
    stringr::str_replace_all(" ", "_") %>%  
    if_else(. == "", as.character(NA), .)
}

clean_up <- function(dataframe) {
  dataframe %>% 
    rename_all(clean_string) %>% 
    mutate_if(is.character, funs(clean_string))%>% 
    distinct()
}


#-------------------location vector-------------------
station.vec <- file.path(CEDR_url,
                         "LivingResources",
                         "TidalPlankton",
                         "Reported",
                         min_date,
                         max_date,
                         phyto_num,
                         "Station") %>%
  fromJSON() %>%
  pull(unique(MonitoringLocationId))
#-----------------------------------------------------
wq_selection_vector <- function(){
  huc8.vec <- file.path(CEDR_url,
                        "LivingResources",
                        "TidalPlankton",
                        "Reported",
                        min_date,
                        max_date,
                        phyto_num,
                        "Huc8"
  ) %>%
    fromJSON() %>%
    pull(unique(HUCEightId))
}

HUC8.vec <- wq_selection_vector()

data.df <- file.path(CEDR_url,
                                  "WaterQuality",
                                  "WaterQuality",

                                  min_date,
                                  max_date,
                                  "6",  #programIds
                                  paste(
                                          #these are the parameter variables declared in Functions and Variables section
                                          MAIN,TRIB
                                          , sep=","),#"7,16,23,24",  #projectIds
                             
                                  "Huc8",
                                  #"Station",
                                  paste(HUC8.vec, collapse = ","),
                                  #paste(station.vec, collapse = ","),

        paste(
SALINITY, DIN, #PO4,
SECCHI, CHLA, PHEO, DOC
                , sep=",")
) %>%

   fromJSON() %>%
   clean_up()

#write raw data, and data soon to be modified to directory
write_csv(data.df, paste0(data_path, "data_raw.csv"))
write_csv(data.df, paste0(data_path, "data_modified.csv"))#,col_names = FALSE)


#read the data soon to be modified
data_modified.df <- data.table::fread(paste0(data_path, "data_modified.csv"),
                                   data.table = FALSE)

#if the column problem is included in the data proceed, otherwise not
if("problem" %in% colnames(data_modified.df))
{
#print("good")

data_problems.df <- data_modified.df %>%
  filter(!is.na(problem )) #%>%

#qq_problems = count(data_problems.df$problem == "qq")

qq_count <- as.numeric(sum(data_problems.df$problem == "qq"))
a_count <- as.numeric(sum(data_problems.df$problem == "a"))


a_string = paste("the number of ", A, "is", a_count)
qq_string = paste("the number of ", QQ, "is", qq_count)


#in function form
count_and_report_problems <-  function(problem_name, problem_description){
  #read in data
  data_modified.df <- data.table::fread(paste0(data_path, "data_modified.csv"),
                                        data.table = FALSE)
  
  #if the column problem is included in the data proceed, otherwise not
  if("problem" %in% colnames(data_modified.df))
  {
    #count problem type
    problem_count <- as.numeric(sum(data_modified.df$problem == as.character(problem_name)))
  
    #report problem type
    problem_report = paste("the number of ", problem_description, "is", problem_count)
    }
}

test <- count_and_report_problems("qq", QQ)
print(test)
}

library(rlist)

all_problems <- list()

print(paste0(report <- count_and_report_problems("a", A),
#list.append(all_problems, report)
"\n ",
report <- count_and_report_problems("b", B)))
# list.append(all_problems, report)
# report <- count_and_report_problems("bb", BB)
# list.append(all_problems, report)
# report <- count_and_report_problems("c", C)
# list.append(all_problems, report)
# report <- count_and_report_problems("d", D)

print(all)

print(typeof(data_modified.df$qualifier))

if("qualifier" %in% colnames(data_modified.df))
{
#print("good")

data_qualifiers.df <- data_modified.df %>%
  #filter(!is.na(qualifier )) 
  filter(qualifier == "g")
}



```

```{r}

library(shiny)
library(shinythemes)
library(shinydashboard)
library(tidyverse)
library(dplyr)
library(jsonlite)
library(ggplot2)
library(rlang)
library(data.table)
library(stringi)
library(Cairo)
library(RcppRoll)
library(tidyr)
library(lubridate)
library(pryr)
library(zoo)
library(sp)
library(leaflet)
library(rgdal)
library(RCurl)
library(curl)

#-------------path to data directory-------------------
data_path <- "data/CEDR/"
#------------------------------------------------------

#-----CEDR api url-------------------------------------
CEDR_url <- "http://datahub.chesapeakebay.net/api.JSON"
#------------------------------------------------------

#-------------------todays date------------------------
todays.date <- format(Sys.Date(), "%m-%d-%Y")
#------------------------------------------------------

#------number for phytoplankton in CEDR api-------------------
phyto_num = "17"
#-------------------------------------------------------------

#this is the minimum date to be included
min_date = "12-13-2018"#1970"#

#this is the maximum date to be included, add the variable todays.date into this variable if you want the most recent data
max_date = todays.date#

CEDR_url <- "http://datahub.chesapeakebay.net/api.JSON"
todays.date <- format(Sys.Date(), "%m-%d-%Y")

MAIN = 7 #Tidal Mainstem Water Quality Monitoring Project
TRIB = 23 #Tidal Tributary Water Quality Monitoring Project

#----------------water quality parameters-----------------------
CLW =23
DIN = 30
DO = 31
DOC = 34
DON = 35
DOP = 36
NH4F = 60
NO23F = 63
NO2F = 65
PC =  71
PH = 73
PN =  77
PO4F = 78
PP =82
SALINITY = 83
SIGMA_T = 88
SO4W = 93
SPCOND = 94
TALK = 100
TDN = 104
TDP = 105
TN = 109
TON = 111
TP = 114
TSS = 116
TURB_NTU = 119
WTEMP = 123

SECCHI = 85
CHLA=21
PHEO=74 
#----------------------------------------------------------------

#--------------------------------data collection problem codes---
A = "LABORATORY ACCIDENT" 
B = "CHEMICAL MATRIX  INTERFERENCE" 
BB  = "TORN FILTER PAD" 
C = "INSTRUMENT FAILURE" 
D = "INSUFFICIENT SAMPLE" 
DD = "SAMPLE SIZE NOT REPORTED (ASSUMED)" 
E = "SAMPLE RECEIVED AFTER HOLDING TIME" 
FF =  "POOR REPLICATION BETWEEN PADS, MEAN REPORTED" 
GG =  "SAMPLE ANALYZED AFTER HOLDING TIME" 
I = "SUSPECT VALUE HAS BEEN VERIFIED CORRECT" 
J =  "INCORRECT SAMPLE FRACTION FOR ANALYSIS" 
JJ = "VOLUME FILTERED NOT RECORDED (ASSUMED)" 
L = "LICOR CALIBRATION OFF BY >= 10% PER YEAR.  USE WITH CALC KD WHERE PROB OF LU,   LS, LB EXIST IN RAW" 
LB = "LICOR CALIBRATION OFF BY >= 10% PER YEAR FOR BOTH AIR AND UPWARD  FACING SENSORS" 
LS =  "LICOR CALIBRATION OFF BY >= 10% PER YEAR FOR AIR SENSOR" 
LU = "LICOR CALIBRATION OFF BY >= 10% PER YEAR FOR UPWARD FACING SENSOR" 
MM = "OVER 20% OF SAMPLE ADHERED TO POUCH AND OUTSIDE OF PAD" 
NN = "PARTICULATES FOUND IN FILTERED SAMPLE"   
P = "PROVISIONAL DATA" 
QQ = "PART EXCEEDS WHOLE VALUE YET DIFFERENCE IS WITHIN ANALYTICAL PRECISION" 
R = "SAMPLE CONTAMINATED" 
RR = "NO SAMPLE RECEIVED"                                                                                   
SS = "SAMPLE REJECTED, HIGH SUSPENDED SEDIMENT CONCENTRATION" 
U = " MATRIX PROBLEM RESULTING FROM THE INTERRELATIONSHIP BETWEEN VARIABLES SUCH AS PH AND AMMONIA"         
V = "SAMPLE RESULTS REJECTED DUE TO QC CRITERIA" 
VV = "STATION WAS NOT SAMPLED DUE TO BAD FIELD CONDITIONS" 
WW = "HIGH OPTICAL DENSITY (750 NM); ACTUAL VALUE RECORDED" 
X = "SAMPLE NOT PRESERVED PROPERLY"
#-----------------------------------------------------------------

clean_string <- function(dataframe) {
  dataframe %>% 
    stringr::str_trim() %>% 
    tolower() %>% 
    stringr::str_replace_all("\\s+", " ") %>% 
    stringr::str_replace_all(" ", "_") %>%  
    if_else(. == "", as.character(NA), .)
}

clean_up <- function(dataframe) {
  dataframe %>% 
    rename_all(clean_string) %>% 
    mutate_if(is.character, funs(clean_string))%>% 
    distinct()
}


#-------------------location vector-------------------
station.vec <- file.path(CEDR_url,
                         "LivingResources",
                         "TidalPlankton",
                         "Reported",
                         min_date,
                         max_date,
                         phyto_num,
                         "Station") %>%
  fromJSON() %>%
  pull(unique(MonitoringLocationId))
#-----------------------------------------------------
wq_selection_vector <- function(){
  huc8.vec <- file.path(CEDR_url,
                        "LivingResources",
                        "TidalPlankton",
                        "Reported",
                        min_date,
                        max_date,
                        phyto_num,
                        "Huc8"
  ) %>%
    fromJSON() %>%
    pull(unique(HUCEightId))
}

HUC8.vec <- wq_selection_vector()

print(HUC8.vec[])

#looking for data from this watershed
#Wicomico River 02080110


huc_data.df <- file.path(CEDR_url,
                                  "WaterQuality",
                                  "WaterQuality",

                                  min_date,
                                  max_date,
                                  "6",  #programIds
                                  paste(
                                          #these are the parameter variables declared in Functions and Variables section
                                          MAIN,TRIB
                                          , sep=","),#"7,16,23,24",  #projectIds
                             
                                  "Huc8",
                                  #"Station",
                                  paste(HUC8.vec, collapse = ","),
                                  #paste(station.vec, collapse = ","),

        paste(
SALINITY, DIN, #PO4,
SECCHI, CHLA, PHEO, DOC
                , sep=",")
) %>%

   fromJSON() %>%
   clean_up()

```

```{r}
wq_huc12_selection_vector <- function(){
  huc8.vec <- file.path(CEDR_url,
                        "LivingResources",
                        "TidalPlankton",
                        "Reported",
                        min_date,
                        max_date,
                        phyto_num,
                        "Huc12"
  ) %>%
    fromJSON() %>%
    pull(unique(HUCTwelveId))
}

HUC12.vec <- wq_huc12_selection_vector()

huc_data.df <- file.path(CEDR_url,
                                  "WaterQuality",
                                  "WaterQuality",

                                  min_date,
                                  max_date,
                                  "6",  #programIds
                                  paste(
                                          #these are the parameter variables declared in Functions and Variables section
                                          MAIN,TRIB
                                          , sep=","),#"7,16,23,24",  #projectIds
                             
                                  "Huc12",
                                  #"Station",
                                  paste(1316L),#HUC8.vec, collapse = ","),
                                  #paste(station.vec, collapse = ","),

        paste(
SALINITY, DIN, #PO4,
SECCHI, CHLA, PHEO, DOC
                , sep=",")
) %>%

   fromJSON() %>%
   clean_up()

#write_csv(huc_data.df, paste0(data_path, "severn_for_andrea.csv"))#,col_names = FALSE)

```

```{r}
library(dplyr)
#-------------path to data directory-------------------
data_path <- "data/CEDR/"
#------------------------------------------------------

data_modified.df <- data.table::fread(paste0(data_path, "severn_for_andrea.csv"),
                                   data.table = FALSE) %>%
  select(station, latitude, longitude) %>%
  unique()

write_csv(data_modified.df, paste0(data_path, "severn_unique_for_andrea.csv"))#,col_names = FALSE)
```

```{r}

count_and_report_problems <-  function(problem_name, problem_description){
  #read in data
  data_modified.df <- data.table::fread(paste0(data_path, "data_modified.csv"),
                                        data.table = FALSE)
  
  #if the column problem is included in the data proceed, otherwise not
  if("problem" %in% colnames(data_modified.df))
  {
    #count problem type
    problem_count <- as.numeric(sum(data_modified.df$problem == as.character(problem_name)))
    
    #report problem type
    problem_report = paste("the number of ",problem_name," = ", problem_description, "is", problem_count)
    return(problem_report)
  }
}

report_all_problems <-  function(){
  
  #read in data
  data_modified.df <- data.table::fread(paste0(data_path, "data_modified.csv"),
                                        data.table = FALSE)
  
#if the column problem is included in the data proceed, otherwise not
  if("problem" %in% colnames(data_modified.df))
  { 

    
    all_problems <- paste0(
    count_and_report_problems("a", A),"\n ",
    count_and_report_problems("b", B),"\n ",
    count_and_report_problems("bb", BB),"\n ",
    count_and_report_problems("c", C),"\n ",
    count_and_report_problems("d", D),"\n ",
    count_and_report_problems("dd", DD),"\n ",
    count_and_report_problems("e", E),"\n ",
    count_and_report_problems("f", FF),"\n ",
    count_and_report_problems("g", GG),"\n ",
    count_and_report_problems("i", I),"\n ",
    count_and_report_problems("j", J),"\n ",
    count_and_report_problems("jj", JJ),"\n ",
    count_and_report_problems("lb", LB),"\n ",
    count_and_report_problems("ls", LS),"\n ",
    count_and_report_problems("lu", LU),"\n ",
    count_and_report_problems("mm", MM),"\n ",
    count_and_report_problems("nn", NN),"\n ",
    count_and_report_problems("p", P),"\n ",
    count_and_report_problems("qq", QQ),"\n ",
    count_and_report_problems("r", R),"\n ",
    count_and_report_problems("rr", RR),"\n ",
    count_and_report_problems("ss", SS),"\n ",
    count_and_report_problems("u", U),"\n ",
    count_and_report_problems("v",V),"\n ",
    count_and_report_problems("vv", VV),"\n ",
    count_and_report_problems("ww", WW),"\n ",
    count_and_report_problems("x", X),"\n "
    )
    
    return(all_problems)
    
  }
}

test <- report_all_problems()
print(test)
```

```{r}

#--------------------------------data collection problem codes---
A = "LABORATORY ACCIDENT" 
B = "CHEMICAL MATRIX  INTERFERENCE" 
BB  = "TORN FILTER PAD" 
C = "INSTRUMENT FAILURE" 
D = "INSUFFICIENT SAMPLE" 
DD = "SAMPLE SIZE NOT REPORTED (ASSUMED)" 
E = "SAMPLE RECEIVED AFTER HOLDING TIME" 
FF =  "POOR REPLICATION BETWEEN PADS, MEAN REPORTED" 
GG =  "SAMPLE ANALYZED AFTER HOLDING TIME" 
I = "SUSPECT VALUE HAS BEEN VERIFIED CORRECT" 
J =  "INCORRECT SAMPLE FRACTION FOR ANALYSIS" 
JJ = "VOLUME FILTERED NOT RECORDED (ASSUMED)" 
L = "LICOR CALIBRATION OFF BY >= 10% PER YEAR.  USE WITH CALC KD WHERE PROB OF LU,   LS, LB EXIST IN RAW" 
LB = "LICOR CALIBRATION OFF BY >= 10% PER YEAR FOR BOTH AIR AND UPWARD  FACING SENSORS" 
LS =  "LICOR CALIBRATION OFF BY >= 10% PER YEAR FOR AIR SENSOR" 
LU = "LICOR CALIBRATION OFF BY >= 10% PER YEAR FOR UPWARD FACING SENSOR" 
MM = "OVER 20% OF SAMPLE ADHERED TO POUCH AND OUTSIDE OF PAD" 
NN = "PARTICULATES FOUND IN FILTERED SAMPLE"   
P = "PROVISIONAL DATA" 
QQ = "PART EXCEEDS WHOLE VALUE YET DIFFERENCE IS WITHIN ANALYTICAL PRECISION" 
R = "SAMPLE CONTAMINATED" 
RR = "NO SAMPLE RECEIVED"                                                                                   
SS = "SAMPLE REJECTED, HIGH SUSPENDED SEDIMENT CONCENTRATION" 
U = " MATRIX PROBLEM RESULTING FROM THE INTERRELATIONSHIP BETWEEN VARIABLES SUCH AS PH AND AMMONIA"         
V = "SAMPLE RESULTS REJECTED DUE TO QC CRITERIA" 
VV = "STATION WAS NOT SAMPLED DUE TO BAD FIELD CONDITIONS" 
WW = "HIGH OPTICAL DENSITY (750 NM); ACTUAL VALUE RECORDED" 
X = "SAMPLE NOT PRESERVED PROPERLY"
#-----------------------------------------------------------------




generate_problem_table <-  function(){
  #read in data
  data_modified.df <- data.table::fread(paste0(data_path, "data_modified.csv"),
                                        data.table = FALSE) %>%
    filter(problem == "qq")
    
    
    mutate(error_code = problem) %>%
    mutate(definition = case_when(problem == "a"~A,
                                  problem == "b"~B,
                                  problem == "bb"~BB,
                                  problem == "c"~C,
                                  problem == "d"~D,
                                  problem == "dd"~DD,
                                  problem == "e"~E,
                                  problem == "ff"~FF,
                                  problem == "gg"~GG,
                                  problem == "i"~I,
                                  problem == "j"~J,
                                  problem == "jj"~JJ,
                                  problem == "lb"~LB,
                                  problem == "ls"~LS,
                                  problem == "lu"~LU,
                                  problem == "mm"~MM,
                                  problem == "nn"~NN,
                                  problem == "p"~P,
                                  problem == "qq"~QQ,
                                  problem == "r"~R,
                                  problem == "rr"~RR,
                                  problem == "ss"~SS,
                                  problem == "u"~U,
                                  problem == "v"~V,
                                  problem == "vv"~VV,
                                  problem == "ww"~WW,
                                  problem == "x"~X,
                                  TRUE~"no problem") ) %>%
    #filter(definition != "no problem") %>%
    group_by(parameter, error_code) #%>%
    #mutate(n = n())
  
  
  
}

test.df <- generate_problem_table()



# problem_count <- as.numeric(sum(data_modified.df$problem == as.character(problem_name)))
```

```{r}

# S------------------------------------------count_problems---------------------------------------------
count_and_report_problems <-  function(problem_name, problem_description){
  #read in data
  data_modified.df <- data.table::fread(paste0(data_path, "data_modified.csv"),
                                        data.table = FALSE)
  
  #if the column problem is included in the data proceed, otherwise not
  if("problem" %in% colnames(data_modified.df))
  {
    #count problem type
    problem_count <- as.numeric(sum(data_modified.df$problem == as.character(problem_name)))
    
    #report problem type
    problem_report = paste("the number of ",problem_name," = ", problem_description, "is", problem_count)
    return(problem_report)
  }
}
# E------------------------------------------count_problems---------------------------------------------



# table_all_problems
test <- count_and_report_problems("a", A)
```

```{r}
table_all_problems <-  function(problem_name, problem_description){
  #read in data
  data_modified.df <- data.table::fread(paste0(data_path, "data_modified.csv"),
                                        data.table = FALSE)
  
  #if the column problem is included in the data proceed, otherwise not
  if("problem" %in% colnames(data_modified.df))
  {
    
    #count problem type
    problem_count <- as.numeric(sum(data_modified.df$problem == as.character(problem_name)))
    
    #report problem type
    problem_report = paste("the number of ",problem_name," = ", problem_description, "is", problem_count)
    
    problem_table.df <- data
    return(problem_table.df)
  }
}
```

```{r}
library(dplyr)
library(tidyr)
library(data.table)
library(lubridate)
library(readxl)
library(jsonlite)
library(rprojroot)
library(ritis)
library(stringr)
library(purrr)
library(lubridate)



project.dir <- rprojroot::find_rstudio_root_file()


clean_string <- function(x) {
  x %>% 
    stringr::str_trim() %>% 
    tolower() %>% 
    stringr::str_replace_all("\\s+", " ") %>% 
    stringr::str_replace_all(" ", "_") %>%  
    if_else(. == "", as.character(NA), .)
}

clean_up <- function(x) {
  x %>% 
    rename_all(clean_string) %>% 
    mutate_if(is.character, funs(clean_string))%>% 
    distinct()
}

url.root <- "http://datahub.chesapeakebay.net/api.JSON"
todays.date <- format(Sys.Date(), "%m-%d-%Y")

min_date = "10-01-2018"#1970"
max_date = todays.date


pico_num = "18"
phyto_num = "17"
chla=21
doc=34 
pheo=74 
salinity=83

#stations
station.vec <- file.path(url.root,
                       "LivingResources",
                       "TidalPlankton",
                       "Reported",
                       min_date,
                       max_date, 
                       phyto_num,
                       "Station") %>% 
  fromJSON() %>% 
  pull(unique(MonitoringLocationId))


###pull phyto data
phyto.df <- file.path(url.root,
                      "LivingResources",
                      "TidalPlankton",
                      "Reported",
                       min_date,
                       max_date, 
                      phyto_num,
                      "Station",
                      paste(station.vec, collapse = ",")) %>%
  fromJSON() %>% 
  clean_up()




phyto.df %>% 
  mutate(reportingvalue = as.character(reportingvalue)) %>% 
data.table::fwrite(file.path(project.dir, "practice/data/3_day_window", "cedr_phyto_taxa_test.csv"))

print(typeof(phyto.df$sampledate))

phyto.df <- phyto.df %>%
  mutate(sampledate=as.Date(sampledate))

print(typeof(phyto.df$sampledate))


###wq data pull
wq.df <- file.path(url.root,
                   "WaterQuality",
                   "WaterQuality",

                   format(min(phyto.df$sampledate) - days(3), "%m-%d-%Y"),
                   format(max(phyto.df$sampledate) + days(3), "%m-%d-%Y"),

                   # min_date,
                   # max_date,
                   "6",  #programIds
                   "7,23",#"7,16,23,24",  #projectIds
                   "station",
                   paste(station.vec, collapse = ","),
                   paste(
                     #these are the parameter variables declared in Functions and Variables section
                     chla,doc,pheo,salinity
                     , sep=",")) %>%
  fromJSON() %>% 
  clean_up()

  data.table::fwrite(wq.df, file.path(project.dir, "practice/data/3_day_window", "cedr_wq_test.csv"))
  
  
#dataframe of unique station/date combos with upper and lower date range added  
data.sub <-  phyto.df %>%
  select(station, sampledate) %>%
  distinct() %>%
  mutate(lower_date = sampledate - lubridate::days(3),
          upper_date = sampledate + lubridate::days(3))




library(parallel)
n.cores <- detectCores() - 1
cl <- makeCluster(n.cores)
clusterExport(cl = cl, varlist = c("wq.df", "data.sub"))
clusterEvalQ(cl, c(library(dplyr))) %>% invisible()


#env.df
test_result.df <- parLapply(cl, 1:nrow(data.sub), function(row.i) {
  
  sub.df <- slice(data.sub, row.i)
  #----------------------------------------------------------------------------
  sub.env <- wq.df %>% 
    filter(station == sub.df$station,
           date >= sub.df$lower_date,
           date <= sub.df$upper_date)
  #----------------------------------------------------------------------------
  if (nrow(sub.env) == 0) return(data.frame(
    station = NA,
    date = NA,
    parameter = NA,
    measurevalue = NA
  ))
  #----------------------------------------------------------------------------
  final.df <- sub.env %>% 
    mutate(date_diff = date - sub.df$sampledate,
           abs_date_diff = abs(date_diff),
           sampledate = sub.df$sampledate) %>% 
    filter(abs_date_diff == min(abs_date_diff))
  #----------------------------------------------------------------------------
  if (nrow(final.df) > 1) {
    final.df <- final.df %>% 
      filter(date == min(date))
  }
  #----------------------------------------------------------------------------
  return(final.df)
}) %>% 
  bind_rows() %>% 
  filter(!is.na(station))

#end env.df

stopCluster(cl)
```

This is a little microcosmic version of the 3 day window problem.  When the code is working properly: The first 4 water quality entries should all be joinable to a single phyto data entry, with the fifth entry having no joinable option and being removed.  
```{r}
library(dplyr)
library(tidyr)
library(data.table)
library(lubridate)
library(readxl)
library(jsonlite)
library(rprojroot)
library(ritis)
library(stringr)
library(purrr)
library(lubridate)

#------------------------------create phyto dataframe--------------
phyto_dates <-  c(as.Date("2019-01-10"),as.Date("2019-01-15"),as.Date("2019-01-20"),as.Date("2019-01-25"),as.Date("2019-01-30"))

phyto_stations <- c("B1", "C2", "C2", "C3", "Z")

phyto_test.df <- data.frame("sampledate" = phyto_dates, "station" = phyto_stations)

#assign upper and lower range for each date
phyto_test.df <- phyto_test.df %>%
    mutate(lower_date = sampledate - lubridate::days(3),
         upper_date = sampledate + lubridate::days(3))
#-------------------------------------------------------------------




#-----------------------create water quality dataframe--------------
wq_dates <-  c(as.Date("2019-01-10"),as.Date("2019-01-12"),as.Date("2019-01-16"),as.Date("2019-02-01"),as.Date("2019-05-30"))

wq_stations <- c("B1", "C2", "C2", "B1", "Z")

wq_parameter <- c("chla", "chla", "chla", "chla", "chla" )

wq_measurevalue <-  c(8.01, 5.43, 9.54, 6.66, 5.19)

wq_test.df <- data.frame("sampledate" = wq_dates, "station" = wq_stations, "parameter" = wq_parameter, "measurevalue" = wq_measurevalue)
#-------------------------------------------------------------------


#--------------------this where the function goes-------------------
#for i in phyto
#for i in wq 
#if i in wq  in range lower to upper
#then i = data 
#until another is closer 
#then replaces i
#until all have been iterated through
#using parallel p?



#-------------------------------------------------------------------



#this is the join that  should attach 4 of the 5 wq entries to phyto entries
#-------------------------------------------------------------------
test.df <- left_join(phyto_test.df, wq_test.df, by = c("station", "sampledate"))
#
```

```{r}
test_fun <- function(phyto_sampledate, date_lower, date_upper, waqu_sampledate, waqu_parameter, waqu_measurevalue, phyto_full) {
  phyto_test <- phyto_full %>%
    mutate(result = "blank")
  
  for(i in phyto_sampledate){

      phyto_test$result[i] = case_when(i >= date_lower[i] && i <= date_upper[i] ~  "pass", TRUE ~ "fail"
                )
      
  }
  return(phyto_test)
}

lapply_test.df <- lapply(phyto_test.df$sampledate, test_fun, date_lower = phyto_test.df$lower_date, date_upper = phyto_test.df$upper_date, waqu_sampledate = wq_test.df$sampledate, waqu_parameter = wq_test.df$parameter, waqu_measurevalue = wq_test.df$measurevalue, phyto_full = phyto_test.df)
```

```{r}
print(test.df$station[4])
```

```{r}
library(dplyr)
library(tidyr)
library(data.table)
library(lubridate)
library(readxl)
library(jsonlite)
library(rprojroot)
library(ritis)
library(stringr)
library(purrr)
library(lubridate)

#------------------------------create phyto dataframe--------------
phyto_dates <-  c(as.Date("2019-01-10"))

phyto_stations <- c("B1")

phyto_test.df <- data.frame("sampledate" = phyto_dates, "station" = phyto_stations)

#assign upper and lower range for each date
phyto_test.df <- phyto_test.df %>%
    mutate(lower_date = sampledate - lubridate::days(3),
         upper_date = sampledate + lubridate::days(3))
#-------------------------------------------------------------------




#-----------------------create water quality dataframe--------------
wq_dates <-  c(as.Date("2019-01-11"),as.Date("2019-01-08"),as.Date("2019-01-16"))

wq_stations <- c("B1", "B1","B1")

wq_parameter <- c("chla", "chla", "chla")

wq_measurevalue <-  c(8.01, 5.43, 9.54)

wq_test.df <- data.frame("sampledate" = wq_dates, "station" = wq_stations, "parameter" = wq_parameter, "measurevalue" = wq_measurevalue)
#-------------------------------------------------------------------
```

```{r}


# final.df <- phyto_test.df %>%
  apple <- "nunya"
  for(phy in phyto_test.df$sampledate){
    for(wq in wq_test.df$sampledate){
      if (
        (
        (wq >= phyto_test.df$lower_date[phy] && wq <= phyto_test.df$upper_date[phy]) &&
        (apple == "nunya") 
      )
        ||
        (
        (wq >= phyto_test.df$lower_date[phy] && wq <= phyto_test.df$upper_date[phy]) &&
        ( abs(phyto_test.df$sampledate[phy] - wq) < abs(phyto_test.df$sampledate[phy] - apple) )
      )
      )
      {apple = wq} 
    }}
print(apple)





```


```{r}
coldate= as.Date(c("2013-08-03", "2013-09-04", "2013-09-08", "2013-09-12", "2013-11-01"));

x = as.Date("2013-10-01")

which(abs(coldate-x) == min(abs(coldate - x)))
```

```{r}
phyto_dates <-  c(as.Date("2019-01-10"), as.Date("2019-01-21"))

test <- function(dates) {
  which(abs(wq_test.df$sampledate - dates) == min(abs(wq_test.df$sampledate - dates)))
  
}

#test(phyto_dates)

lapply(phyto_dates, test)

# for (phy in phyto_dates){
# which(abs(wq_test.df$sampledate - phy) == min(abs(wq_test.df$sampledate - phy)))
# }
```
```{r}
library(dplyr)
library(tidyr)
library(data.table)
library(lubridate)
library(readxl)
library(jsonlite)
library(rprojroot)
library(ritis)
library(stringr)
library(purrr)
library(lubridate)

#------------------------------create phyto dataframe--------------
phyto_dates <-  c(as.Date("2019-01-10"),as.Date("2019-01-15"),as.Date("2019-01-20"),as.Date("2019-01-25"),as.Date("2019-01-30"))

phyto_stations <- c("B1", "C2", "C2", "C3", "Z")

event.df <- data.frame("sampledate" = phyto_dates, "station" = phyto_stations)
#-------------------------------------------------------------------


#-----------------------create water quality dataframe--------------
wq_dates <-  c(as.Date("2019-01-10"),as.Date("2019-01-12"),as.Date("2019-01-16"),as.Date("2019-02-01"),as.Date("2019-05-30"))

wq_stations <- c("B1", "C2", "C2", "B1", "Z")

wq_parameter <- c("chla", "chla", "chla", "pheo", "pheo" )

wq_measurevalue <-  c(8.01, 5.43, 9.54, 6.66, 5.19)

wq.df <- data.frame("sampledate" = wq_dates, "station" = wq_stations, "parameter" = wq_parameter, "measurevalue" = wq_measurevalue)
#-------------------------------------------------------------------

event.sub <- event.df %>% 
  select(station, sampledate) %>% 
  distinct() %>% 
  mutate(sampledate = as.Date(sampledate)) %>%
  mutate(lower_date = sampledate - lubridate::days(3),
         upper_date = sampledate + lubridate::days(3))

library(parallel)
n.cores <- detectCores() - 1
cl <- makeCluster(n.cores)
clusterExport(cl = cl, varlist = c("wq.df", "event.sub"))
clusterEvalQ(cl, c(library(dplyr))) %>% invisible()

env.df <- parLapply(cl, 1:nrow(event.sub), function(row.i) {

  sub.df <- slice(event.sub, row.i)
  # #----------------------------------------------------------------------------
  # sub.env <- wq.df %>%
  #   filter(station == sub.df$station,
  #          date >= sub.df$lower_date,
  #          date <= sub.df$upper_date)
  # #----------------------------------------------------------------------------
  # if (nrow(sub.env) == 0) return(data.frame(
  #   station = NA,
  #   date = NA,
  #   parameter = NA,
  #   measurevalue = NA
  # ))
  # #----------------------------------------------------------------------------
  # final.df <- sub.env %>%
  #   mutate(date_diff = date - sub.df$sampledate,
  #          abs_date_diff = abs(date_diff),
  #          sampledate = sub.df$sampledate) %>%
  #   filter(abs_date_diff == min(abs_date_diff))
  # #----------------------------------------------------------------------------
  # if (nrow(final.df) > 1) {
  #   final.df <- final.df %>%
  #     filter(date == min(date))
  # }
  # #----------------------------------------------------------------------------
  # return(final.df)
  
})


# %>%
#   bind_rows() %>%
#   filter(!is.na(station))

stopCluster(cl)


#is this necessary?
env.wide <- env.df %>% 
  spread(parameter, measurevalue)

event.df <- left_join(event.df, env.df, by = c("station", "sampledate"))

```


```{r}
library(dplyr)
library(tidyr)
library(data.table)
library(lubridate)
library(readxl)
library(jsonlite)
library(rprojroot)
library(ritis)
library(stringr)
library(purrr)
library(lubridate)

#------------------------------create phyto dataframe--------------
phyto_dates <-  c(as.Date("2019-01-10"),as.Date("2019-01-15"),as.Date("2019-01-20"),as.Date("2019-01-25"),as.Date("2019-01-30"))

phyto_stations <- c("B1", "C2", "C2", "C3", "Z")

event.df <- data.frame("sampledate" = phyto_dates, "station" = phyto_stations,
                       stringsAsFactors = FALSE
                       )
#-------------------------------------------------------------------

#-----------------------create water quality dataframe--------------
wq_dates <-  c(as.Date("2019-01-10"),as.Date("2019-01-12"),as.Date("2019-01-16"),as.Date("2019-02-01"),as.Date("2019-01-30"))

wq_stations <- c("B1", "C2", "C2", "B1", "Z")

wq_parameter <- c("chla", "chla", "chla", "pheo", "pheo" )

wq_measurevalue <-  c(8.01, 5.43, 9.54, 6.66, 5.19)

wq.df <- data.frame("sampledate" = wq_dates, "station" = wq_stations, "parameter" = wq_parameter, 
                    "measurevalue" = wq_measurevalue, stringsAsFactors = FALSE)
#-------------------------------------------------------------------
event.sub <- event.df %>% 
  select(station, sampledate) %>% 
  distinct() %>% 
  mutate(sampledate = as.Date(sampledate)) %>%
  mutate(lower_date = sampledate - lubridate::days(3),
         upper_date = sampledate + lubridate::days(3))

# event.df <-factor(event.df, levels = 4)
# wq.df <- factor(wq.df,levels = 4)

levels(event.df)
levels(wq.df)

library(parallel)
n.cores <- detectCores() - 1
cl <- makeCluster(n.cores)
clusterExport(cl = cl, varlist = c("wq.df", "event.sub"))
clusterEvalQ(cl, c(library(dplyr))) %>% invisible()

env.df <- parLapply(cl, 1:nrow(event.sub), function(row.i) {

  sub.df <- slice(event.sub, row.i)
  #----------------------------------------------------------------------------
  sub.env <- wq.df %>%
    filter(station == sub.df$station,
           sampledate >= sub.df$lower_date,
           sampledate <= sub.df$upper_date
           )
  #----------------------------------------------------------------------------
  if (nrow(sub.env) == 0) return(data.frame(
    station = NA,
    sampledate = NA,
    parameter = NA,
    measurevalue = NA
  ))
  #----------------------------------------------------------------------------
  final.df <- sub.env %>%
    mutate(date_diff = sampledate - sub.df$sampledate,
           abs_date_diff = abs(date_diff),
           sampledate = sub.df$sampledate) %>%
    filter(abs_date_diff == min(abs_date_diff))
  #----------------------------------------------------------------------------
  if (nrow(final.df) > 1) {
    final.df <- final.df %>%
      filter(sampledate == min(sampledate))
  }
  #----------------------------------------------------------------------------
  return(final.df)
  
}) %>%
  bind_rows() %>%
  filter(!is.na(station))

stopCluster(cl)


#is this necessary?
env.wide <- env.df %>%
  spread(parameter, measurevalue)

event.df <- left_join(event.df, env.df, by = c("station", "sampledate"))

```

```{r}
library(dplyr)
library(tidyr)
library(data.table)
library(lubridate)
library(readxl)
library(jsonlite)
library(rprojroot)
library(ritis)
library(stringr)
library(purrr)
library(lubridate)

data_path <- "data/CEDR/"

  data_modified.df <- data.table::fread(paste0(data_path, "data_modified.csv"),
                                        data.table = FALSE) #%>%
    
    data_modified.df <-  data_modified.df %>%
    spread(parameter, measurevalue)
    
    data_modified.df <-  data_modified.df %>%
    mutate(chla_mean =  mean(chla, na.rm = TRUE))
    
    
```

```{r}
library(RCurl)
library(stringr)
library(curl)
library(jsonlite)

#script to get most recent virginia drought map


test = "//waterservices.usgs.gov/nwis/iv/?format=waterml,2.0&sites=01646500&parameterCd=00060,00065&siteStatus=all"

test2 = "http://waterservices.usgs.gov/nwis/iv/?sites=01646500&parameterCd=00060,00065"

#json version
test3 = "http://waterservices.usgs.gov/nwis/iv/?format=json&sites=01646500&parameterCd=00060,00065&siteStatus=all"

test.df <- fromJSON(test3)

test_curl <- curl::curl(test2)
tmp <- tempfile()
test_curl < - curl_download(test, tmp)



library(XML)
library(methods)

# Give the input file name to the function.
result <- xmlParse(file = "input.xml")

# Print the result.
print(result)

# out <- readLines(test_curl)
# print(out[1:30])
```

```{r}
library(jsonlite)
library(dplyr)
library(tidyr)

#json version
test3 = "http://waterservices.usgs.gov/nwis/iv/?format=json&sites=01646500&parameterCd=00060,00065&siteStatus=all"

usgs_base = "http://waterservices.usgs.gov/nwis/iv/"

usgs_format = "format=json&sites=01646500"

usgs_parameter = "parameterCd=00060,00065" 

usgs_site ="siteStatus=all"




test <- file.path(test3) %>%
  fromJSON()

test2 <- paste0(usgs_base, "?",usgs_format, "&", usgs_parameter, "&", usgs_site ) %>%
  fromJSON()



#ugh <- data.frame(test2[["value"]]["timeSeries"])
ugh1 <- data.frame(test2[["value"]][["timeSeries"]][["values"]][[1]])
ugh2 <- data.frame(test2[["value"]][["timeSeries"]][["values"]][2])




#########################################


# url.root <- test3#"http://datahub.chesapeakebay.net/api.JSON"
# todays.date <- format(Sys.Date(), "%m-%d-%Y")
# 
# station.vec <- file.path(url.root,
#                        "LivingResources",
#                        "TidalPlankton",
#                        "Reported",
#                        min_date,
#                        max_date, 
#                        phyto_num,
#                        "Station") %>% 
#   fromJSON() %>% 
#   pull(unique(MonitoringLocationId))
```

```{r}
library(EGRET)
library(dataRetrieval)
Daily <- getNWISDaily("01491000", "00060","1979-10-01", "2010-09-30")
Daily <- EGRET::readNWISDaily("01491000", "00060","1979-10-01", "2010-09-30")
```

```{r}
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyr)
library(lubridate)

project.dir <- rprojroot::find_rstudio_root_file()

data_path <- "data/CEDR/"



data_last.df <- data.table::fread(paste0(data_path, "cedr_phyto_tweak.csv"),
                                        data.table = FALSE)

last_download_date <- max(data_last.df$sampledate)
```

