---
title: "R_practice4"
author: "Luke Vawter"
date: "9/23/2020"
output: html_document
---
```{r}
project.dir <- rprojroot::find_rstudio_root_file()

active_path <- "/data/ACTIVE/"

test_path <- (paste0(project.dir,active_path, "active_data.csv"))

print(test_path)

```



```{r}
# library(shiny)
# library(shinythemes)
# library(shinydashboard)
# library(ggplot2)
# library(dplyr)
# library(rlang)
# library(data.table)
# library(stringi)
# library(Cairo)
# library(RcppRoll)
# library(tidyr)
library(lubridate)
# library(pryr)
# library(zoo)
# library(sp)
# library(png)
# library(leaflet)
# library(rgdal)
library(RCurl)
# library(tidyverse)
# library(curl)

test_date = as.Date(today())

# url_to_date = (paste0('https://mde.maryland.gov/programs/Water/droughtinformation/Currentconditions/PublishingImages/DroughtGraphsStarting2019jan31/Drought',format(test_date,"20%y-%m-%d"),'.png'))
url_to_date = ('https://mde.maryland.gov/programs/Water/droughtinformation/Currentconditions/PublishingImages/DroughtGraphsStarting2019jan31/Drought2020-08-31.png')
    
#tests if
test <- url.exists(url_to_date)
    
print(test)

md_drought_map <- readPNG(url_to_date)


        box(
          title = NULL,#"MARYLAND DROUGHT STATUS",
          width = NULL,#6,
          height = 220,
          htmlOutput(outputId="MD_title"),
          box(tags$img(alt="Drought Status Map:2019-05-31",
                       src= md_drought_map,
                       style="width:250px;height:150px;border:0;")
            #leafletOutput("mymap", height =140, width =300)
            )
          )


```
```{r}
library(shiny)
library(shinythemes)
library(shinydashboard)
library(png)
library(lubridate)

date_today0 <- as.Date(today())
date_yest <- date_today0 - 1

print(date_yest)

#url to site with map
url_to_date = (paste0('https://mde.maryland.gov/programs/Water/droughtinformation/Currentconditions/PublishingImages/DroughtGraphsStarting2019jan31/Drought',date_today0,'.png'))
  
               
#https://mde.maryland.gov/programs/Water/droughtinformation/Currentconditions/PublishingImages/DroughtGraphsStarting2019jan31/Drought2020-08-31.png
project.dir <- rprojroot::find_rstudio_root_file()

map_path <- "/Practice/www/"#data/drought_map/"

md_map_full_path <- (paste0(project.dir,map_path, "md_map.png"))

map_download = download.file(url_to_date, md_map_full_path, mode = 'wb')
  
md_drought_map <- readPNG(md_map_full_path)


ui <- #fluidPage(
  dashboardPage(skin = "blue" ,
  dashboardHeader(title = "Drought Map",
                  .list = NULL),
  dashboardSidebar(),
  dashboardBody(
  
  
          box(
          title = NULL,#"MARYLAND DROUGHT STATUS",
          width = NULL,#6,
          height = 220,
          htmlOutput(outputId="MD_title"),
          box(tags$img(alt="Drought Status Map:2019-05-31",
                       src= md_drought_map,
                       style="width:250px;height:150px;border:0;")
            #leafletOutput("mymap", height =140, width =300)
            )
          )
          
  )
  
)

server <- function(input, output)({

})


shinyApp(ui, server)
```

```{r}
library(tidyr)
library(dplyr)
library(shiny)
library(shinythemes)
library(shinydashboard)
library(png)
library(lubridate)

 #this is the url that contains all the map files
  map_url_head ='http://deq1.bse.vt.edu/drought/state/images/maps/'
  
  #creates an object to open 
  test <- curl::curl(map_url_head)
  
  #open and read said object
  open(test)
  out <- readLines(test)
  
  map.df <- as.data.frame(out, row.names = NULL, optional = TRUE)#, col.names = cols )
  colnames(map.df) <- c("html")
  
  ######patterns for finding string fragments for retrieving map
  pattern_id <- "(\\d{10,})"
  pattern_date <- "(\\d{4}[-]\\d{2}[-]\\d{2})"
  pattern_time <- "(\\d{2}[:]\\d{2})"
  ################
  
  #executing above patterns with extract to generate dataframes containing relevant fragments of string
  id.df <- extract(map.df,html, c("id"), regex = pattern_id)
  date.df <- extract(map.df,html, c("date"), regex = pattern_date)
  time.df <- extract(map.df,html, c("time"), regex = pattern_time)
  
  #bind dataframes into one
  maps_full.df <- cbind(map.df,id.df, date.df,time.df)
  
  #arrange so that most current file is at the top
  maps_full.df <- arrange(maps_full.df, desc(date),desc(time))
  
  #grabs the top item
  map_url_id = maps_full.df$id[1]
  
  #concatinate all relevant parts into a retrievable url
  map_url_full = as.character(paste0(map_url_head,"imageMapFile", map_url_id,".png"))
  
  map_path <- "/Practice/www/"
  
  #place to download png
  va_map_full_path <- (paste0(project.dir,map_path, "va_map.png"))

  #downloading png
  map_download = download.file(map_url_full, va_map_full_path, mode = 'wb')
  
  va_drought_map <- readPNG(va_map_full_path)#map_url_full)
  
  
  

  
server <- function(input, output, session) {
 
output$boxes  <- renderUI({
          box(
          title = NULL,#"MARYLAND DROUGHT STATUS",
          width = NULL,#6,
          height = 220,
          htmlOutput(outputId="VA_title"),
          box(tags$img(alt="Drought Status Map:2019-05-31",
                       src= va_drought_map,
                       style="width:250px;height:150px;border:0;")
            #leafletOutput("mymap", height =140, width =300)
            )
          )
})}
  
  
  
server <- function(input, output)({

  ui <- dashboardPage(
  dashboardHeader(title = "break me"
                  ),
  dashboardSidebar(dateInput("date", "enter a date")
                   ),
  dashboardBody(htmlOutput(outputId = "boxes")
  )
)
  
})
  
  
#   ui <- #fluidPage(
#   dashboardPage(skin = "blue" ,
#   dashboardHeader(title = "Drought Map",
#                   .list = NULL),
#   dashboardSidebar(),
#   dashboardBody(
#   
#   
#           box(
#           title = NULL,#"MARYLAND DROUGHT STATUS",
#           width = NULL,#6,
#           height = 220,
#           htmlOutput(outputId="VA_title"),
#           box(tags$img(alt="Drought Status Map:2019-05-31",
#                        src= va_drought_map,
#                        style="width:250px;height:150px;border:0;")
#             #leafletOutput("mymap", height =140, width =300)
#             )
#           )
#           
#   )
#   
# )
# 
# server <- function(input, output)({
# 
# })


shinyApp(ui, server)
```
```{r}
library(tidyr)
library(dplyr)
library(shiny)
library(shinythemes)
library(shinydashboard)
library(png)
library(lubridate)

  project.dir <- rprojroot::find_rstudio_root_file()

 #this is the url that contains all the map files
  map_url_head ='http://deq1.bse.vt.edu/drought/state/images/maps/'
  
  #creates an object to open 
  test <- curl::curl(map_url_head)
  
  #open and read said object
  open(test)
  out <- readLines(test)
  
  map.df <- as.data.frame(out, row.names = NULL, optional = TRUE)#, col.names = cols )
  colnames(map.df) <- c("html")
  
  ######patterns for finding string fragments for retrieving map
  pattern_id <- "(\\d{10,})"
  pattern_date <- "(\\d{4}[-]\\d{2}[-]\\d{2})"
  pattern_time <- "(\\d{2}[:]\\d{2})"
  ################
  
  #executing above patterns with extract to generate dataframes containing relevant fragments of string
  id.df <- extract(map.df,html, c("id"), regex = pattern_id)
  date.df <- extract(map.df,html, c("date"), regex = pattern_date)
  time.df <- extract(map.df,html, c("time"), regex = pattern_time)
  
  #bind dataframes into one
  maps_full.df <- cbind(map.df,id.df, date.df,time.df)
  
  #arrange so that most current file is at the top
  maps_full.df <- arrange(maps_full.df, desc(date),desc(time))
  
  #grabs the top item
  map_url_id = maps_full.df$id[1]
  
  #concatinate all relevant parts into a retrievable url
  map_url_full = as.character(paste0(map_url_head,"imageMapFile", map_url_id,".png"))
  
  map_path <- "/Practice/www/"
  
  #place to download png
  va_map_full_path <- (paste0(project.dir,map_path, "va_map.png"))

  #downloading png
  map_download = download.file(map_url_full, va_map_full_path, mode = 'wb')
  
  va_drought_map <- readPNG(va_map_full_path)#map_url_full)
  
  print(va_drought_map)
  
  rasterImage(va_drought_map)
  
shinyApp(ui, server)
```

```{r}
library(tidyr)
library(dplyr)

cedr_path <- "data/CEDR/"

active_data.df <- data.table::fread(paste0(cedr_path, "data", "_raw", ".csv"),
                                    data.table = FALSE)

  metrics_list <- active_data.df %>%
     dplyr::select(parameter) %>%
     unique %>%
     pull(.,parameter)
     #.$paramter
  
print(metrics_list)
print(typeof(metrics_list))

if("panda" %in% metrics_list){print("success")}
```
```{r}
library(lubridate)
library(dplyr)

#this is the minimum date to be included
min_date = #"01-01-1970"
#"01-01-2018"
"2018-01-01"


#-------------------todays date-----------------------
todays.date <- #format(Sys.Date(), "%m-%d-%Y")
  format(Sys.Date(), "%Y-%m-%d")
  #"2020-01-01"
#------------------------------------------------------

#this is the maximum date to be included, add the variable todays.date into this variable if you want the most recent data
max_date = todays.date#

cedr_path <- "data/CEDR/"

raw_data <- "cedr_raw_huc12.csv"

our_data <- paste0(cedr_path,raw_data)

active_data.df <- data.table::fread(our_data,
                                        header = TRUE,
                                        data.table = FALSE) %>%
      #filter by date range selection
  filter(as.Date(sampledate) >= as.Date(min_date), as.Date(sampledate) <= as.Date(max_date))
    # filter(as.Date(sampledate) >= as.Date(input$date_range[1]), as.Date(sampledate) <= as.Date(input$date_range[2])) %>%
      
    # #filter by metric selection
    # filter(parameter %in% input$select_metric) #%>%

```

```{r}
# library(EGRET)
library(dataRetrieval)

#siteNumber can be found at:  https://maps.waterdata.usgs.gov/mapper/index.html

# Site Number: 01649500
# Site Name: NORTHEAST BRANCH

# nwis_data <- readNWISDaily(siteNumber = "01646500", parameterCd = "00060", startDate = "2019-01-01",
#   endDate = "2020-01-01", verbose = TRUE, interactive = NULL, convert = TRUE)
# 
# nwis_data2 <- readNWISDaily(siteNumber = "01594440", parameterCd = "00060", startDate = "2019-01-01",
#   endDate = "2020-01-01", verbose = TRUE, interactive = NULL, convert = TRUE)

# INFO <- readNWISInfo("01646500","00060")

siteNumbers <- c("01594440","01646500") 
site_info <- readNWISsite(siteNumbers)
# 
# INFO2<- readNWISInfo('01594440',"00618")

params<-whatNWISdata(siteNumber ="01646500", service="all", statCd="all")
                     #service="dv", statCd="00003")
 
parameterINFO <- readNWISpCode(parameterCd="00060")


```

```{r}

####this is a script for formatting RSM ticket reports

####the Team columns need to be changed to Team1 and Team2 before this script will run
library(dplyr)
library(data.table)

project.dir <- rprojroot::find_rstudio_root_file()

ticket_report.df <- data.table::fread(file.path(project.dir, 
                                         "Practice/data/ticket_reports/RSM_ticket_report_4_2021_input.csv"),
                            data.table = FALSE,
                            #colClasses = col.class.vec,
                            na.strings = "")



ticket_report_new.df <- ticket_report.df %>%
  mutate(Priority = case_when(
    Priority == 'common/images/../../common/images/infoIcons/urgencyColors/Red.gif' ~ "EMERGENCY",
    Priority == 'common/images/../../common/images/infoIcons/urgencyColors/orange.gif' ~ "priority",
    Priority == 'common/images/../../common/images/infoIcons/urgencyColors/pink.gif' ~ "medium",
    Priority == 'common/images/../../common/images/infoIcons/urgencyColors/yellow.gif' ~ "next available",
    Priority == 'common/images/../../common/images/infoIcons/urgencyColors/White.gif' ~ "as scheduled",
    Priority == 'common/images/../../common/images/infoIcons/urgencyColors/purple.gif' ~ "open for all",
    TRUE ~ Priority
  ))%>%
  filter(`Summary Description` != 'MyAnalytics | Wellbeing Edition'
)

#average_closed_ticket_time.df <- ticket_report_new.df %>%
    
# 
#   
#open_closed_tickets.df <- ticket_report_new.df %>%
    # mutate(iris, sepal = Sepal.Length + Sepal. Width) 
    # mutate(Open = )
    # Open <- as.numeric(sum(ticket_report_new.df$Display == "open"))
    # Closed <- as.numeric(sum(ticket_report_new.df$Display == "open"))
#mutate(carriers, n = n())
      
#   
# Ticket_by_type <- ticket_report_new.df %>%
  
  
    write.csv(ticket_report_new.df, paste0(project.dir, 
                                         "/Practice/data/ticket_reports/RSM_ticket_report_4_2021_output.csv"))
```


```{r}
library(SSN)

 data_path ='data/SSN/Colorado.ssn'
 
 # 'C:\Users\lvawter\Documents\R_scripts\workflow\Practice\data\SSN\Colorado.ssn'

# Function to standardize variables
stand <- function(x) { (x-mean(x))/(2*sd(x))}

# Function to standardize a prediction dataset based on the fitting dataset 
stdpreds <- function(newset,originalset) {
	xnames <- colnames(newset)
	sx <- matrix(rep(NA,ncol(newset)*nrow(newset)),nrow=nrow(newset))
	for(i in 1:ncol(newset)) {
		var <- with(originalset,get(xnames[i]))
		sx[,i] <- (newset[,i]-mean(var))/(2*sd(var))
		}
	colnames(sx) <- colnames(newset)
	return(sx)
}

test.ssn <- importSSN(data_path, predpts = "preds", o.write = T)

plot(test.ssn, lwdLineCol = "addfunccol", lwdLineEx = 8,
lineCol = "blue", cex = 2, xlab = "x-coordinate",
ylab = "y-coordinate", pch = 1)

# pred_test = getPreds(test.ssn, pred.type = 'pred')


#had to check memory and then adjust memory limit
memory.size()
memory.limit()
memory.limit(size=200000)

#dist_mat_test = 
createDistMat(test.ssn, o.write=T, predpts = 'preds', amongpreds=T)
# ssn, predpts = NULL, o.write = FALSE, amongpreds = FALSE)

SD_test <- getStreamDistMat(test.ssn)
# print(SD_test)
# Create raw torgegram
test_torg <- Torgegram(test.ssn,"STREAM_AUG",nlag=20)

test.df<- getSSNdata.frame(test.ssn)
test.lm <- lm(STREAM_AUG ~ ELEV + CANOPY + SLOPE + PRECIP + CUMDRAINAG + Y_COORD + NLCD11PC + GLACIER + BFI + TAILWATER + Air_Aug + Flow_Aug, data=test.df)
summary(test.lm)

# Standardize continuous covariates and add factor for year
continuous <- test.df[,c(10:18,20:21)]
cont.s <- apply(continuous,2,stand)
colnames(cont.s) <- c("elev","canopy","slope","precip","drainage","lat","water","glacier","bfi","airtemp","flow")
test.df.s <- data.frame(test.df,cont.s)
test.df.s$yearf <- factor(test.df.s$SAMPLEYEAR)
test_cov <- putSSNdata.frame(test.df.s,test.ssn,"Obs")



testAe <- summary(test.lm)$coefficients
backtrans <- testAe[-c(1,10),1:2]/(2*sapply(continuous[,-8],sd))
esttable <- cbind(rbind(testAe[1,1:2],backtrans[1:8,],testAe[10,1:2],backtrans[9:10,]),testAe)
rownames(esttable) <- rownames(testAe)
write.csv(esttable,"test_aspatialestimates.csv")

# Aspatial performance
predictA <- predict(test.lm)
sqrt(mean((predictA-test.df.s$STREAM_AUG)^2))
print(predictA)
# 1.885

# Examine correlations
library(ellipse)
cor(cont.s)
# jpeg("corrplot.jpg")
plotcorr(cor(cont.s),type="lower")
dev.off()

# Get VIFs from linear model as an indicator of multicollinearity
library(car)
vif(test.lm)
#      ELEV     CANOPY      SLOPE     PRECIP CUMDRAINAG    Y_COORD   NLCD11PC 
#  2.399760   1.584861   1.351117   1.577125   1.909954   2.585873   1.305458 
#   GLACIER        BFI  TAILWATER    Air_Aug   Flow_Aug 
#  1.048601   1.255432   1.323322   1.048707   1.051820 

# Standardize preds based on obs and add factor for year- prednorth
testpreddf <- getSSNdata.frame(test.ssn, "preds")
contpred <- testpreddf[,c(10:18,20:21)]
contpred.s <- stdpreds(contpred,continuous)
colnames(contpred.s) <- c("elev","canopy","slope","precip","drainage","lat","water","glacier","bfi","airtemp","flow")
testpreddf.s <- data.frame(testpreddf,contpred.s)
testpreddf.s$yearf <- factor(testpreddf.s$SAMPLEYEAR)
test_pred <- putSSNdata.frame(testpreddf.s,test_cov,"preds")
```

```{r}
library(xlsx)
library(rJava)
library(dplyr)

path = 'data/SSN/META_SITES-EDGES.xlsx'

wb     <- loadWorkbook(path)
sheet1 <- getSheets(wb)[[1]]

# get all rows
rows  <- getRows(sheet1)
cells <- getCells(rows)
```

attempt to log into Data Portal with curl
```{r}
library(curl)

    login_url = 'https://icprbcoop.org/user/login'

    data_url = 'https://icprbcoop.org/products/wma_withdrawals_private.csv'
    
    wma_pub_url = 'https://icprbcoop.org/products/wma_withdrawals_public.csv'
   
    
    userid <- "admin1"
    passwd <- "CsSaAsLv123!!"
    credentials = paste0(userid,':',passwd)
   
    h = new_handle(verbose = TRUE)
    handle_setopt(handle = h, httpauth =1, userpwd = credentials)
    
    # wma <- curl::curl_fetch_memory(wma_pub_url,  handle = h)
    # jsonlite::fromJSON(rawToChar(wma$content))
    
    login_attempt <- curl::curl_fetch_memory(login_url, handle = h)
    jsonlite::fromJSON(rawToChar(login_attempt$content))
    # wma <- curl::curl_fetch_memory(data_url,  handle = h)
    # jsonlite::fromJSON(rawToChar(wma$content))

    
```
```{r}
library(RCurl)

#Set your browsing links 
loginurl = 'https://icprbcoop.org/user/login'
dataurl  = 'https://icprbcoop.org/products/wma_withdrawals_private.csv'

#Set user account data and agent
pars=list(
     Username="admin1",
     Password="CsSaAsLv123!!"
)
agent="Mozilla/5.0" #or whatever 

#Set RCurl pars
curl = getCurlHandle()
curlSetOpt(cookiejar="cookies.txt",  useragent = agent, followlocation = TRUE, curl=curl)
#Also if you do not need to read the cookies. 
#curlSetOpt(  cookiejar="", useragent = agent, followlocation = TRUE, curl=curl)

#Post login form
html=postForm(loginurl, .params = pars, curl=curl)
# print(html)

#Go wherever you want
html=getURL(dataurl, curl=curl)
print(html)

# html=getURL('https://icprbcoop.org/user/1', curl=curl)

# out <- read.csv(textConnection(html))
# print(out)


#Start parsing your page
# matchref=gregexpr("... my regexp ...", html)

#... .... ...

#Clean up. This will also print the cookie file
rm(curl)
gc()
```

```{r}
library(RCurl)
dataurl  = 'https://icprbcoop.org/products/wma_withdrawals_public.csv'

agent="Mozilla/5.0" #or whatever 

curl = getCurlHandle()
curlSetOpt(cookiejar="cookies.txt",  useragent = agent, followlocation = TRUE, curl=curl)

html=getURL(dataurl, curl=curl)

print(html)

# out <- read.csv(textConnection(html))
# print(out)
```

```{r}
library(httr)

# r <- GET('https://icprbcoop.org/')
r <- GET('https://icprbcoop.org/fw/')
# r <- GET('https://google.com/')
print(cookies(r))

xVYgJCBeaXGwEQTMuUL4X2caHzBalzO1t29zfW--RJU
```
```{r}
library(httr)
url<-"https://icprbcoop.org/products/wma_withdrawals_private.csv"

r <-GET(url, 
    set_cookies( "SSESS26ffc8725d1747c99af20df19a5da588"))###"xVYgJCBeaXGwEQTMuUL4X2caHzBalzO1t29zfW--RJU"))
r_content= content(as="text",r)
print(r_content)



####public
url2<-"https://icprbcoop.org/products/wma_withdrawals_public.csv"

r_public <-GET(url2)

r_pub_content= content(as="text",r_public)
print(r_pub_content)
```

```{r}
library(httr)
url_control<-"https://icprb.sharepoint.com/sites/COOPPortal/_layouts/15/Doc.aspx?sourcedoc={a90d9594-9041-469c-8c33-79b9cd286b5c}&action=edit&wd=target%28backups.one%7C127bfce4-1455-4f75-a577-eccb62c3ce64%2FBackups%20old%7Ce21a084a-42ae-4cdf-85a7-e48e32e081c2%2F%29&wdorigin=NavigationUrl"

r_control <-GET(url)
r_control_content= content(as="text",r)
print(r_control_content)
```
wma_hidden_test
```{r}
library(digest)
library(lubridate)

test_time = ymd_hms(now("GMT"))
print(test_time)

test_month = month.abb[month(test_time)]
print(test_month)

hash_stamp = paste0(test_month,day(test_time),year(test_time), sprintf("%02d",hour(test_time))) ##converts hour to two digit format 
print(hash_stamp)
test_hash = toString(paste0(hash_stamp,'wikajfkgha'))
print(test_hash)
test_hash_full = digest::digest(test_hash, algo="md5", serialize = FALSE)
print(test_hash_full)
# Sep12202216wikajfkgha

library(httr)

file_url <- paste0("https://icprbcoop.org/products/wma_withdrawals_hidden.csv?password=",test_hash_full)
print(file_url)

wma_csv <-GET(file_url)

wma_csv_content <-content(as="text",wma_csv)

print(wma_csv_content)
###1f38dddfeebcb6cbae72c7b62ca1cf77

```

control test
```{r}

wma_csv <-GET("https://icprbcoop.org/products/wma_withdrawals_public.csv")
# 
# # print(wma_csv)
wma_csv_content <-content(as="text",wma_csv)
print(wma_csv_content)
url2<-"https://icprbcoop.org/products/wma_withdrawals_public.csv"


```

data_view hidden test
```{r}
library(digest)
library(lubridate)
library(httr)

test_time = ymd_hms(now("GMT"))
print(test_time)
test_month = month.abb[month(test_time)]
print(test_month)
hash_stamp = paste0(test_month,day(test_time),year(test_time), sprintf("%02d",hour(test_time)))
print(hash_stamp)
hash = toString(paste0(hash_stamp,'djasokdmas'))
print(hash)

hash = digest::digest(hash, algo="md5", serialize = FALSE)
print("hash is: ...")
print(hash)
site_url = "https://icprbcoop.org/products/data_view_hidden?password="

selection = paste0("&wssc_usable_storage_patuxent=wssc_usable_storage_patuxent",
"&wssc_usable_storage_seneca=wssc_usable_storage_seneca",
"&fw_usable_storage_occoquan=fw_usable_storage_occoquan")

date_submit ="&startdate=09%2F12%2F2022&enddate=09%2F19%2F2022&format=graph&submit=Submit"
file_url <- paste0(site_url,hash,selection,date_submit)
# print(file_url)

#acaf4343a553abdf2773c3cdf7f554dd
  
print(file_url)
data_view <-GET(file_url)
data_view_content <-content(as="text",data_view)
print(data_view_content)

storage_local_daily_bg_df <- data.table::fread(data_view_content)

# password=4682e91a861af0cbf54a63258b1a346d&wssc_usable_storage_patuxent=wssc_usable_storage_patuxent&wssc_usable_storage_seneca=wssc_usable_storage_seneca&fw_usable_storage_occoquan=fw_usable_storage_occoquan&startdate=09%2F12%2F2022&enddate=09%2F19%2F2022&format=graph&submit=Submit"

# https://icprbcoop.org/products/data_view_public?wssc_usable_storage_patuxent=wssc_usable_storage_patuxent&wssc_usable_storage_seneca=wssc_usable_storage_seneca&fw_usable_storage_occoquan=fw_usable_storage_occoquan&startdate=09%2F12%2F2022&enddate=09%2F19%2F2022&format=graph&submit=Submit
# 
# https://icprbcoop.org/products/data_view_hidden?password=574a66c0dc25de031d010bf3099141b4&wssc_usable_storage_patuxent=wssc_usable_storage_patuxent&wssc_usable_storage_seneca=wssc_usable_storage_seneca&fw_usable_storage_occoquan=fw_usable_storage_occoquan&startdate=09%2F12%2F2022&enddate=09%2F19%2F2022&format=graph&submit=Submit
```

```{r}
test_time = ymd_hms(now("GMT"))

test_month = month.abb[month(test_time)]
print(test_month)
print(year(test_time))
print(sprintf("%02d",hour(test_time)))
sprintf("%02d",day(test_time))

hash_stamp = paste0(test_month,sprintf("%02d",day(test_time)),year(test_time), sprintf("%02d",hour(test_time)))
##converts hour and day to two digit format 

# hash_stamp = paste0(test_month,day(test_time),year(test_time), sprintf("%02d",hour(test_time)))

test_hash = toString(paste0(hash_stamp,'wikajfkgha'))
print(test_hash)
test_hash_full = digest::digest(test_hash, algo="md5", serialize = FALSE)
print(test_hash_full)

site_url = "https://icprbcoop.org/products/wma_withdrawals_hidden.csv?password="
full_url = paste0(site_url,test_hash_full)
print(full_url)
library(RCurl)
print(url.exists(full_url))
# print(url.exists("https://youtube.com"))
#wma hash a71d3967da217fa58ddb78388f60d8b5

#a71d3967da217fa58ddb78388f60d8b5
# r=Nov1202218wikajfkgha
# Drupal=Nov01202218wikajfkgha
```

```{r}

library(digest)
library(lubridate)
library(httr)
library(RCurl)

test_time = ymd_hms(now("GMT"))
# print(test_time)
test_month = month.abb[month(test_time)]
# print(test_month)
hash_stamp = paste0(test_month,sprintf("%02d",day(test_time)),year(test_time), sprintf("%02d",hour(test_time)))
# hash_stamp = paste0(test_month,day(test_time),year(test_time), sprintf("%02d",hour(test_time)))

print(hash_stamp)
hash = toString(paste0(hash_stamp,'djasokdmas'))
# print(hash)

hash = digest::digest(hash, algo="md5", serialize = FALSE)
# print("hash is: ...")
# print(hash)
site_url = "https://icprbcoop.org/products/data_view_hidden?password="

selection = paste0("&wssc_usable_storage_patuxent=wssc_usable_storage_patuxent",
"&wssc_usable_storage_seneca=wssc_usable_storage_seneca",
"&fw_usable_storage_occoquan=fw_usable_storage_occoquan")

date_submit ="&startdate=09%2F12%2F2022&enddate=09%2F19%2F2022&format=csv&submit=Submit"
file_url_full <- paste0(site_url,hash,selection,date_submit)
file_url_part <- paste0(site_url,hash)
print(file_url_full)
# print(url.exists(file_url_part))
# print(url.exists(file_url_full))
# print(file_url)

#acaf4343a553abdf2773c3cdf7f554dd
  
# print(file_url_part)
data_view <-GET(file_url_full)
data_view_content <-content(as="text",data_view)
# print(data_view_content)

storage_local_daily_bg_df <- data.table::fread(data_view_content)
print(storage_local_daily_bg_df)

# read.file <- function (file.name) {
#   require(data.table)
#   file <- try(fread(file.name))
#   if (class(file) == "try-error") {
#     cat("Caught an error with data portal data, using local data.\n")
#     file <- 1
#     print(class(file))
#   }else{file <- 2}
#   file
# }
# 
# data_view_file <- read.file(data_view_content)
# print(data_view_file)



```





